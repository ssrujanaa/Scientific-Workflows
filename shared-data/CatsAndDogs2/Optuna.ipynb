{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo pip3 install keras==2.1.5 tensorflow==1.13.1 numpy pandas pillow opencv--python sklearn optuna scikit-image ray ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - ETA: 11 - ETA: 29 - ETA: 26 - ETA: 22 - ETA: 19 - ETA: 17 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 10s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 432 samples, validate on 48 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428/432 [============================>.] - ETA: 1:22 - loss: 1.2013 - acc: 0.0000e+0 - ETA: 1:08 - loss: 2.3978 - acc: 0.2500    - ETA: 1:04 - loss: 2.4664 - acc: 0.166 - ETA: 1:03 - loss: 2.2635 - acc: 0.125 - ETA: 1:01 - loss: 2.2457 - acc: 0.100 - ETA: 1:01 - loss: 2.0100 - acc: 0.166 - ETA: 1:00 - loss: 1.8186 - acc: 0.214 - ETA: 58s - loss: 1.6809 - acc: 0.250 - ETA: 57s - loss: 1.6037 - acc: 0.27 - ETA: 56s - loss: 1.5365 - acc: 0.25 - ETA: 56s - loss: 1.4624 - acc: 0.27 - ETA: 55s - loss: 1.3772 - acc: 0.33 - ETA: 54s - loss: 1.4293 - acc: 0.30 - ETA: 54s - loss: 1.3526 - acc: 0.35 - ETA: 53s - loss: 1.3356 - acc: 0.36 - ETA: 53s - loss: 1.2732 - acc: 0.40 - ETA: 52s - loss: 1.3014 - acc: 0.38 - ETA: 52s - loss: 1.2488 - acc: 0.41 - ETA: 51s - loss: 1.1918 - acc: 0.44 - ETA: 51s - loss: 1.2487 - acc: 0.42 - ETA: 51s - loss: 1.2107 - acc: 0.45 - ETA: 50s - loss: 1.1901 - acc: 0.45 - ETA: 50s - loss: 1.1768 - acc: 0.43 - ETA: 50s - loss: 1.1649 - acc: 0.43 - ETA: 49s - loss: 1.1431 - acc: 0.44 - ETA: 49s - loss: 1.1282 - acc: 0.44 - ETA: 49s - loss: 1.1076 - acc: 0.46 - ETA: 48s - loss: 1.1119 - acc: 0.46 - ETA: 48s - loss: 1.1054 - acc: 0.44 - ETA: 48s - loss: 1.1148 - acc: 0.43 - ETA: 47s - loss: 1.0978 - acc: 0.43 - ETA: 47s - loss: 1.0766 - acc: 0.45 - ETA: 47s - loss: 1.0577 - acc: 0.46 - ETA: 46s - loss: 1.0624 - acc: 0.45 - ETA: 46s - loss: 1.0399 - acc: 0.47 - ETA: 46s - loss: 1.0249 - acc: 0.48 - ETA: 46s - loss: 1.0146 - acc: 0.48 - ETA: 45s - loss: 1.0006 - acc: 0.50 - ETA: 45s - loss: 1.0238 - acc: 0.48 - ETA: 45s - loss: 1.0081 - acc: 0.50 - ETA: 44s - loss: 0.9855 - acc: 0.51 - ETA: 44s - loss: 0.9630 - acc: 0.52 - ETA: 44s - loss: 0.9427 - acc: 0.53 - ETA: 44s - loss: 0.9960 - acc: 0.52 - ETA: 43s - loss: 0.9864 - acc: 0.52 - ETA: 43s - loss: 0.9798 - acc: 0.52 - ETA: 43s - loss: 0.9701 - acc: 0.53 - ETA: 43s - loss: 0.9629 - acc: 0.53 - ETA: 42s - loss: 0.9464 - acc: 0.54 - ETA: 42s - loss: 0.9645 - acc: 0.53 - ETA: 42s - loss: 0.9561 - acc: 0.53 - ETA: 41s - loss: 0.9516 - acc: 0.53 - ETA: 41s - loss: 0.9628 - acc: 0.52 - ETA: 41s - loss: 0.9553 - acc: 0.52 - ETA: 41s - loss: 0.9533 - acc: 0.52 - ETA: 40s - loss: 0.9438 - acc: 0.53 - ETA: 40s - loss: 0.9344 - acc: 0.54 - ETA: 40s - loss: 0.9298 - acc: 0.54 - ETA: 40s - loss: 0.9259 - acc: 0.54 - ETA: 39s - loss: 0.9127 - acc: 0.55 - ETA: 39s - loss: 0.9093 - acc: 0.54 - ETA: 39s - loss: 0.9018 - acc: 0.55 - ETA: 39s - loss: 0.8947 - acc: 0.56 - ETA: 39s - loss: 0.8859 - acc: 0.57 - ETA: 38s - loss: 0.8788 - acc: 0.57 - ETA: 38s - loss: 0.8725 - acc: 0.58 - ETA: 38s - loss: 0.8726 - acc: 0.58 - ETA: 38s - loss: 0.8762 - acc: 0.58 - ETA: 37s - loss: 0.8725 - acc: 0.58 - ETA: 37s - loss: 0.8662 - acc: 0.59 - ETA: 37s - loss: 0.8581 - acc: 0.59 - ETA: 37s - loss: 0.8656 - acc: 0.59 - ETA: 36s - loss: 0.8598 - acc: 0.59 - ETA: 36s - loss: 0.8548 - acc: 0.60 - ETA: 36s - loss: 0.8463 - acc: 0.60 - ETA: 35s - loss: 0.8357 - acc: 0.61 - ETA: 35s - loss: 0.8409 - acc: 0.61 - ETA: 35s - loss: 0.8396 - acc: 0.60 - ETA: 35s - loss: 0.8350 - acc: 0.61 - ETA: 34s - loss: 0.8327 - acc: 0.61 - ETA: 34s - loss: 0.8322 - acc: 0.60 - ETA: 34s - loss: 0.8361 - acc: 0.59 - ETA: 34s - loss: 0.8428 - acc: 0.59 - ETA: 33s - loss: 0.8406 - acc: 0.59 - ETA: 33s - loss: 0.8337 - acc: 0.60 - ETA: 33s - loss: 0.8310 - acc: 0.59 - ETA: 33s - loss: 0.8267 - acc: 0.60 - ETA: 33s - loss: 0.8219 - acc: 0.60 - ETA: 32s - loss: 0.8156 - acc: 0.61 - ETA: 32s - loss: 0.8191 - acc: 0.61 - ETA: 32s - loss: 0.8186 - acc: 0.60 - ETA: 32s - loss: 0.8136 - acc: 0.61 - ETA: 31s - loss: 0.8191 - acc: 0.60 - ETA: 31s - loss: 0.8154 - acc: 0.60 - ETA: 31s - loss: 0.8134 - acc: 0.60 - ETA: 31s - loss: 0.8087 - acc: 0.60 - ETA: 30s - loss: 0.8055 - acc: 0.61 - ETA: 30s - loss: 0.8043 - acc: 0.61 - ETA: 30s - loss: 0.8032 - acc: 0.61 - ETA: 29s - loss: 0.7987 - acc: 0.61 - ETA: 29s - loss: 0.7959 - acc: 0.61 - ETA: 29s - loss: 0.7964 - acc: 0.60 - ETA: 29s - loss: 0.7944 - acc: 0.60 - ETA: 28s - loss: 0.7915 - acc: 0.61 - ETA: 28s - loss: 0.8029 - acc: 0.60 - ETA: 28s - loss: 0.7966 - acc: 0.60 - ETA: 28s - loss: 0.7951 - acc: 0.60 - ETA: 27s - loss: 0.7943 - acc: 0.60 - ETA: 27s - loss: 0.7927 - acc: 0.60 - ETA: 27s - loss: 0.7915 - acc: 0.60 - ETA: 27s - loss: 0.7951 - acc: 0.60 - ETA: 26s - loss: 0.7905 - acc: 0.61 - ETA: 26s - loss: 0.7873 - acc: 0.61 - ETA: 26s - loss: 0.7851 - acc: 0.61 - ETA: 26s - loss: 0.7839 - acc: 0.61 - ETA: 25s - loss: 0.7791 - acc: 0.62 - ETA: 25s - loss: 0.7778 - acc: 0.61 - ETA: 25s - loss: 0.7734 - acc: 0.62 - ETA: 25s - loss: 0.7699 - acc: 0.62 - ETA: 24s - loss: 0.7674 - acc: 0.62 - ETA: 24s - loss: 0.7665 - acc: 0.62 - ETA: 24s - loss: 0.7643 - acc: 0.62 - ETA: 24s - loss: 0.7629 - acc: 0.62 - ETA: 23s - loss: 0.7578 - acc: 0.62 - ETA: 23s - loss: 0.7523 - acc: 0.63 - ETA: 23s - loss: 0.7465 - acc: 0.63 - ETA: 23s - loss: 0.7550 - acc: 0.63 - ETA: 22s - loss: 0.7520 - acc: 0.63 - ETA: 22s - loss: 0.7521 - acc: 0.63 - ETA: 22s - loss: 0.7489 - acc: 0.63 - ETA: 22s - loss: 0.7449 - acc: 0.64 - ETA: 21s - loss: 0.7416 - acc: 0.64 - ETA: 21s - loss: 0.7362 - acc: 0.64 - ETA: 21s - loss: 0.7310 - acc: 0.64 - ETA: 21s - loss: 0.7314 - acc: 0.64 - ETA: 20s - loss: 0.7269 - acc: 0.65 - ETA: 20s - loss: 0.7225 - acc: 0.65 - ETA: 20s - loss: 0.7242 - acc: 0.65 - ETA: 20s - loss: 0.7246 - acc: 0.65 - ETA: 19s - loss: 0.7254 - acc: 0.65 - ETA: 19s - loss: 0.7222 - acc: 0.65 - ETA: 19s - loss: 0.7226 - acc: 0.65 - ETA: 19s - loss: 0.7209 - acc: 0.65 - ETA: 18s - loss: 0.7224 - acc: 0.65 - ETA: 18s - loss: 0.7209 - acc: 0.65 - ETA: 18s - loss: 0.7200 - acc: 0.65 - ETA: 18s - loss: 0.7196 - acc: 0.64 - ETA: 17s - loss: 0.7219 - acc: 0.64 - ETA: 17s - loss: 0.7192 - acc: 0.65 - ETA: 17s - loss: 0.7197 - acc: 0.65 - ETA: 16s - loss: 0.7185 - acc: 0.64 - ETA: 16s - loss: 0.7151 - acc: 0.65 - ETA: 16s - loss: 0.7150 - acc: 0.65 - ETA: 16s - loss: 0.7105 - acc: 0.65 - ETA: 15s - loss: 0.7064 - acc: 0.65 - ETA: 15s - loss: 0.7021 - acc: 0.65 - ETA: 15s - loss: 0.7113 - acc: 0.65 - ETA: 15s - loss: 0.7131 - acc: 0.64 - ETA: 14s - loss: 0.7114 - acc: 0.64 - ETA: 14s - loss: 0.7088 - acc: 0.65 - ETA: 14s - loss: 0.7067 - acc: 0.65 - ETA: 14s - loss: 0.7038 - acc: 0.65 - ETA: 13s - loss: 0.7031 - acc: 0.65 - ETA: 13s - loss: 0.7000 - acc: 0.65 - ETA: 13s - loss: 0.6977 - acc: 0.65 - ETA: 13s - loss: 0.6970 - acc: 0.65 - ETA: 12s - loss: 0.6935 - acc: 0.65 - ETA: 12s - loss: 0.6911 - acc: 0.66 - ETA: 12s - loss: 0.6974 - acc: 0.65 - ETA: 11s - loss: 0.6952 - acc: 0.66 - ETA: 11s - loss: 0.6922 - acc: 0.66 - ETA: 11s - loss: 0.6903 - acc: 0.66 - ETA: 11s - loss: 0.6920 - acc: 0.66 - ETA: 10s - loss: 0.6909 - acc: 0.66 - ETA: 10s - loss: 0.6900 - acc: 0.66 - ETA: 10s - loss: 0.6895 - acc: 0.66 - ETA: 10s - loss: 0.6862 - acc: 0.66 - ETA: 9s - loss: 0.6826 - acc: 0.6657 - ETA: 9s - loss: 0.6789 - acc: 0.667 - ETA: 9s - loss: 0.6850 - acc: 0.666 - ETA: 9s - loss: 0.6870 - acc: 0.663 - ETA: 8s - loss: 0.6884 - acc: 0.662 - ETA: 8s - loss: 0.6870 - acc: 0.663 - ETA: 8s - loss: 0.6848 - acc: 0.665 - ETA: 8s - loss: 0.6841 - acc: 0.664 - ETA: 7s - loss: 0.6826 - acc: 0.666 - ETA: 7s - loss: 0.6806 - acc: 0.668 - ETA: 7s - loss: 0.6773 - acc: 0.670 - ETA: 7s - loss: 0.6741 - acc: 0.672 - ETA: 6s - loss: 0.6791 - acc: 0.671 - ETA: 6s - loss: 0.6763 - acc: 0.672 - ETA: 6s - loss: 0.6794 - acc: 0.669 - ETA: 5s - loss: 0.6768 - acc: 0.671 - ETA: 5s - loss: 0.6737 - acc: 0.672 - ETA: 5s - loss: 0.6784 - acc: 0.669 - ETA: 5s - loss: 0.6813 - acc: 0.665 - ETA: 4s - loss: 0.6823 - acc: 0.665 - ETA: 4s - loss: 0.6815 - acc: 0.664 - ETA: 4s - loss: 0.6805 - acc: 0.665 - ETA: 4s - loss: 0.6794 - acc: 0.667 - ETA: 3s - loss: 0.6765 - acc: 0.669 - ETA: 3s - loss: 0.6742 - acc: 0.670 - ETA: 3s - loss: 0.6734 - acc: 0.670 - ETA: 3s - loss: 0.6704 - acc: 0.671 - ETA: 2s - loss: 0.6679 - acc: 0.673 - ETA: 2s - loss: 0.6667 - acc: 0.674 - ETA: 2s - loss: 0.6649 - acc: 0.676 - ETA: 2s - loss: 0.6620 - acc: 0.677 - ETA: 1s - loss: 0.6590 - acc: 0.679 - ETA: 1s - loss: 0.6621 - acc: 0.678 - ETA: 1s - loss: 0.6602 - acc: 0.680 - ETA: 1s - loss: 0.6583 - acc: 0.681 - ETA: 0s - loss: 0.6595 - acc: 0.680 - ETA: 0s - loss: 0.6575 - acc: 0.6822"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - ETA: 0s - loss: 0.6570 - acc: 0.681 - 64s 148ms/step - loss: 0.6549 - acc: 0.6829 - val_loss: 0.4217 - val_acc: 0.8750\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428/432 [============================>.] - ETA: 1:12 - loss: 0.1652 - acc: 1.000 - ETA: 1:13 - loss: 0.1157 - acc: 1.000 - ETA: 1:18 - loss: 0.3455 - acc: 0.833 - ETA: 1:21 - loss: 0.3545 - acc: 0.875 - ETA: 1:24 - loss: 0.4754 - acc: 0.800 - ETA: 1:19 - loss: 0.4171 - acc: 0.833 - ETA: 1:18 - loss: 0.4209 - acc: 0.785 - ETA: 1:16 - loss: 0.3883 - acc: 0.812 - ETA: 1:14 - loss: 0.5882 - acc: 0.722 - ETA: 1:12 - loss: 0.6230 - acc: 0.700 - ETA: 1:10 - loss: 0.5863 - acc: 0.727 - ETA: 1:09 - loss: 0.5758 - acc: 0.750 - ETA: 1:07 - loss: 0.5687 - acc: 0.730 - ETA: 1:06 - loss: 0.5536 - acc: 0.750 - ETA: 1:05 - loss: 0.5640 - acc: 0.733 - ETA: 1:04 - loss: 0.5804 - acc: 0.718 - ETA: 1:03 - loss: 0.6006 - acc: 0.676 - ETA: 1:02 - loss: 0.6338 - acc: 0.638 - ETA: 1:01 - loss: 0.6333 - acc: 0.631 - ETA: 1:01 - loss: 0.6258 - acc: 0.650 - ETA: 1:00 - loss: 0.6061 - acc: 0.666 - ETA: 59s - loss: 0.5896 - acc: 0.681 - ETA: 58s - loss: 0.5968 - acc: 0.65 - ETA: 58s - loss: 0.5735 - acc: 0.66 - ETA: 57s - loss: 0.5728 - acc: 0.66 - ETA: 56s - loss: 0.5633 - acc: 0.67 - ETA: 56s - loss: 0.5468 - acc: 0.68 - ETA: 55s - loss: 0.5600 - acc: 0.67 - ETA: 55s - loss: 0.5435 - acc: 0.68 - ETA: 54s - loss: 0.5458 - acc: 0.68 - ETA: 54s - loss: 0.5425 - acc: 0.69 - ETA: 53s - loss: 0.5403 - acc: 0.68 - ETA: 53s - loss: 0.5335 - acc: 0.69 - ETA: 52s - loss: 0.5222 - acc: 0.70 - ETA: 52s - loss: 0.5104 - acc: 0.71 - ETA: 51s - loss: 0.5135 - acc: 0.70 - ETA: 51s - loss: 0.5244 - acc: 0.70 - ETA: 50s - loss: 0.5146 - acc: 0.71 - ETA: 50s - loss: 0.5117 - acc: 0.71 - ETA: 50s - loss: 0.5210 - acc: 0.71 - ETA: 49s - loss: 0.5101 - acc: 0.71 - ETA: 49s - loss: 0.5085 - acc: 0.71 - ETA: 48s - loss: 0.5021 - acc: 0.72 - ETA: 48s - loss: 0.5026 - acc: 0.72 - ETA: 48s - loss: 0.4962 - acc: 0.73 - ETA: 47s - loss: 0.4886 - acc: 0.73 - ETA: 47s - loss: 0.5086 - acc: 0.73 - ETA: 46s - loss: 0.5069 - acc: 0.73 - ETA: 46s - loss: 0.5144 - acc: 0.73 - ETA: 46s - loss: 0.5064 - acc: 0.74 - ETA: 45s - loss: 0.5107 - acc: 0.73 - ETA: 45s - loss: 0.5221 - acc: 0.73 - ETA: 45s - loss: 0.5169 - acc: 0.73 - ETA: 44s - loss: 0.5148 - acc: 0.73 - ETA: 44s - loss: 0.5073 - acc: 0.73 - ETA: 44s - loss: 0.5021 - acc: 0.74 - ETA: 44s - loss: 0.4959 - acc: 0.74 - ETA: 43s - loss: 0.4891 - acc: 0.75 - ETA: 43s - loss: 0.4911 - acc: 0.74 - ETA: 43s - loss: 0.4878 - acc: 0.75 - ETA: 42s - loss: 0.4889 - acc: 0.74 - ETA: 42s - loss: 0.5294 - acc: 0.73 - ETA: 42s - loss: 0.5222 - acc: 0.73 - ETA: 42s - loss: 0.5258 - acc: 0.73 - ETA: 41s - loss: 0.5208 - acc: 0.73 - ETA: 41s - loss: 0.5246 - acc: 0.73 - ETA: 41s - loss: 0.5300 - acc: 0.73 - ETA: 41s - loss: 0.5239 - acc: 0.73 - ETA: 40s - loss: 0.5181 - acc: 0.73 - ETA: 40s - loss: 0.5147 - acc: 0.74 - ETA: 40s - loss: 0.5179 - acc: 0.73 - ETA: 39s - loss: 0.5119 - acc: 0.74 - ETA: 39s - loss: 0.5102 - acc: 0.73 - ETA: 39s - loss: 0.5090 - acc: 0.73 - ETA: 39s - loss: 0.5093 - acc: 0.73 - ETA: 38s - loss: 0.5114 - acc: 0.73 - ETA: 38s - loss: 0.5064 - acc: 0.73 - ETA: 38s - loss: 0.5014 - acc: 0.73 - ETA: 37s - loss: 0.5032 - acc: 0.73 - ETA: 37s - loss: 0.5022 - acc: 0.73 - ETA: 37s - loss: 0.4968 - acc: 0.74 - ETA: 37s - loss: 0.4948 - acc: 0.74 - ETA: 36s - loss: 0.4988 - acc: 0.74 - ETA: 36s - loss: 0.4939 - acc: 0.74 - ETA: 36s - loss: 0.5018 - acc: 0.74 - ETA: 35s - loss: 0.5001 - acc: 0.74 - ETA: 35s - loss: 0.5023 - acc: 0.74 - ETA: 35s - loss: 0.5031 - acc: 0.74 - ETA: 35s - loss: 0.5074 - acc: 0.74 - ETA: 34s - loss: 0.5111 - acc: 0.73 - ETA: 34s - loss: 0.5058 - acc: 0.73 - ETA: 34s - loss: 0.5007 - acc: 0.73 - ETA: 33s - loss: 0.5211 - acc: 0.73 - ETA: 33s - loss: 0.5167 - acc: 0.73 - ETA: 33s - loss: 0.5133 - acc: 0.73 - ETA: 32s - loss: 0.5158 - acc: 0.73 - ETA: 32s - loss: 0.5124 - acc: 0.73 - ETA: 32s - loss: 0.5141 - acc: 0.73 - ETA: 32s - loss: 0.5122 - acc: 0.73 - ETA: 31s - loss: 0.5079 - acc: 0.74 - ETA: 31s - loss: 0.5061 - acc: 0.74 - ETA: 31s - loss: 0.5026 - acc: 0.74 - ETA: 31s - loss: 0.5030 - acc: 0.74 - ETA: 30s - loss: 0.5001 - acc: 0.74 - ETA: 30s - loss: 0.4992 - acc: 0.74 - ETA: 30s - loss: 0.4970 - acc: 0.74 - ETA: 30s - loss: 0.4928 - acc: 0.74 - ETA: 29s - loss: 0.4997 - acc: 0.74 - ETA: 29s - loss: 0.4967 - acc: 0.74 - ETA: 29s - loss: 0.4988 - acc: 0.74 - ETA: 29s - loss: 0.4971 - acc: 0.74 - ETA: 28s - loss: 0.4988 - acc: 0.74 - ETA: 28s - loss: 0.4947 - acc: 0.74 - ETA: 28s - loss: 0.4916 - acc: 0.75 - ETA: 28s - loss: 0.4890 - acc: 0.75 - ETA: 28s - loss: 0.4854 - acc: 0.75 - ETA: 27s - loss: 0.4999 - acc: 0.74 - ETA: 27s - loss: 0.4972 - acc: 0.75 - ETA: 27s - loss: 0.4943 - acc: 0.75 - ETA: 26s - loss: 0.5036 - acc: 0.74 - ETA: 26s - loss: 0.5048 - acc: 0.74 - ETA: 26s - loss: 0.5007 - acc: 0.74 - ETA: 25s - loss: 0.5016 - acc: 0.74 - ETA: 25s - loss: 0.5014 - acc: 0.74 - ETA: 25s - loss: 0.4989 - acc: 0.74 - ETA: 25s - loss: 0.4969 - acc: 0.75 - ETA: 24s - loss: 0.4958 - acc: 0.75 - ETA: 24s - loss: 0.4943 - acc: 0.75 - ETA: 24s - loss: 0.4936 - acc: 0.75 - ETA: 24s - loss: 0.4950 - acc: 0.75 - ETA: 23s - loss: 0.4922 - acc: 0.75 - ETA: 23s - loss: 0.4890 - acc: 0.75 - ETA: 23s - loss: 0.4859 - acc: 0.75 - ETA: 22s - loss: 0.4824 - acc: 0.75 - ETA: 22s - loss: 0.4883 - acc: 0.75 - ETA: 22s - loss: 0.4879 - acc: 0.75 - ETA: 22s - loss: 0.4846 - acc: 0.75 - ETA: 21s - loss: 0.4852 - acc: 0.75 - ETA: 21s - loss: 0.4827 - acc: 0.75 - ETA: 21s - loss: 0.4809 - acc: 0.76 - ETA: 20s - loss: 0.4788 - acc: 0.76 - ETA: 20s - loss: 0.4758 - acc: 0.76 - ETA: 20s - loss: 0.4734 - acc: 0.76 - ETA: 20s - loss: 0.4728 - acc: 0.76 - ETA: 19s - loss: 0.4742 - acc: 0.76 - ETA: 19s - loss: 0.4772 - acc: 0.76 - ETA: 19s - loss: 0.4747 - acc: 0.76 - ETA: 18s - loss: 0.4725 - acc: 0.76 - ETA: 18s - loss: 0.4694 - acc: 0.76 - ETA: 18s - loss: 0.4700 - acc: 0.76 - ETA: 18s - loss: 0.4673 - acc: 0.76 - ETA: 17s - loss: 0.4669 - acc: 0.76 - ETA: 17s - loss: 0.4645 - acc: 0.76 - ETA: 17s - loss: 0.4617 - acc: 0.76 - ETA: 17s - loss: 0.4595 - acc: 0.77 - ETA: 16s - loss: 0.4625 - acc: 0.76 - ETA: 16s - loss: 0.4690 - acc: 0.76 - ETA: 16s - loss: 0.4662 - acc: 0.76 - ETA: 15s - loss: 0.4645 - acc: 0.76 - ETA: 15s - loss: 0.4618 - acc: 0.76 - ETA: 15s - loss: 0.4616 - acc: 0.77 - ETA: 15s - loss: 0.4604 - acc: 0.77 - ETA: 14s - loss: 0.4602 - acc: 0.77 - ETA: 14s - loss: 0.4594 - acc: 0.77 - ETA: 14s - loss: 0.4568 - acc: 0.77 - ETA: 13s - loss: 0.4578 - acc: 0.77 - ETA: 13s - loss: 0.4551 - acc: 0.77 - ETA: 13s - loss: 0.4548 - acc: 0.77 - ETA: 13s - loss: 0.4548 - acc: 0.78 - ETA: 12s - loss: 0.4555 - acc: 0.77 - ETA: 12s - loss: 0.4530 - acc: 0.78 - ETA: 12s - loss: 0.4518 - acc: 0.78 - ETA: 11s - loss: 0.4502 - acc: 0.78 - ETA: 11s - loss: 0.4563 - acc: 0.78 - ETA: 11s - loss: 0.4537 - acc: 0.78 - ETA: 11s - loss: 0.4563 - acc: 0.78 - ETA: 10s - loss: 0.4560 - acc: 0.78 - ETA: 10s - loss: 0.4553 - acc: 0.78 - ETA: 10s - loss: 0.4536 - acc: 0.78 - ETA: 9s - loss: 0.4512 - acc: 0.7861 - ETA: 9s - loss: 0.4498 - acc: 0.787 - ETA: 9s - loss: 0.4487 - acc: 0.788 - ETA: 9s - loss: 0.4466 - acc: 0.789 - ETA: 8s - loss: 0.4450 - acc: 0.790 - ETA: 8s - loss: 0.4427 - acc: 0.791 - ETA: 8s - loss: 0.4419 - acc: 0.793 - ETA: 8s - loss: 0.4447 - acc: 0.791 - ETA: 7s - loss: 0.4425 - acc: 0.792 - ETA: 7s - loss: 0.4405 - acc: 0.793 - ETA: 7s - loss: 0.4422 - acc: 0.792 - ETA: 6s - loss: 0.4400 - acc: 0.793 - ETA: 6s - loss: 0.4400 - acc: 0.794 - ETA: 6s - loss: 0.4382 - acc: 0.795 - ETA: 6s - loss: 0.4384 - acc: 0.796 - ETA: 5s - loss: 0.4362 - acc: 0.797 - ETA: 5s - loss: 0.4342 - acc: 0.798 - ETA: 5s - loss: 0.4342 - acc: 0.797 - ETA: 5s - loss: 0.4332 - acc: 0.798 - ETA: 4s - loss: 0.4327 - acc: 0.799 - ETA: 4s - loss: 0.4333 - acc: 0.797 - ETA: 4s - loss: 0.4355 - acc: 0.796 - ETA: 3s - loss: 0.4334 - acc: 0.797 - ETA: 3s - loss: 0.4319 - acc: 0.798 - ETA: 3s - loss: 0.4306 - acc: 0.799 - ETA: 3s - loss: 0.4286 - acc: 0.800 - ETA: 2s - loss: 0.4267 - acc: 0.801 - ETA: 2s - loss: 0.4259 - acc: 0.801 - ETA: 2s - loss: 0.4267 - acc: 0.800 - ETA: 1s - loss: 0.4253 - acc: 0.801 - ETA: 1s - loss: 0.4236 - acc: 0.802 - ETA: 1s - loss: 0.4220 - acc: 0.803 - ETA: 1s - loss: 0.4209 - acc: 0.804 - ETA: 0s - loss: 0.4262 - acc: 0.802 - ETA: 0s - loss: 0.4256 - acc: 0.8037"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - ETA: 0s - loss: 0.4301 - acc: 0.800 - 66s 152ms/step - loss: 0.4281 - acc: 0.8009 - val_loss: 1.0307 - val_acc: 0.5417\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/432 [============================>.] - ETA: 55s - loss: 2.5357 - acc: 0.50 - ETA: 55s - loss: 1.6883 - acc: 0.50 - ETA: 54s - loss: 1.1613 - acc: 0.66 - ETA: 54s - loss: 1.1580 - acc: 0.62 - ETA: 53s - loss: 0.9609 - acc: 0.70 - ETA: 53s - loss: 0.8480 - acc: 0.75 - ETA: 53s - loss: 0.7492 - acc: 0.78 - ETA: 53s - loss: 0.7388 - acc: 0.75 - ETA: 53s - loss: 0.6797 - acc: 0.77 - ETA: 52s - loss: 0.6137 - acc: 0.80 - ETA: 52s - loss: 0.5919 - acc: 0.81 - ETA: 52s - loss: 0.5663 - acc: 0.83 - ETA: 51s - loss: 0.5267 - acc: 0.84 - ETA: 51s - loss: 0.4953 - acc: 0.85 - ETA: 51s - loss: 0.4643 - acc: 0.86 - ETA: 50s - loss: 0.4622 - acc: 0.84 - ETA: 50s - loss: 0.4440 - acc: 0.85 - ETA: 50s - loss: 0.4204 - acc: 0.86 - ETA: 50s - loss: 0.4474 - acc: 0.84 - ETA: 49s - loss: 0.4258 - acc: 0.85 - ETA: 49s - loss: 0.4070 - acc: 0.85 - ETA: 49s - loss: 0.4466 - acc: 0.81 - ETA: 48s - loss: 0.4295 - acc: 0.82 - ETA: 48s - loss: 0.4449 - acc: 0.81 - ETA: 48s - loss: 0.4325 - acc: 0.82 - ETA: 48s - loss: 0.4200 - acc: 0.82 - ETA: 48s - loss: 0.4102 - acc: 0.83 - ETA: 47s - loss: 0.4033 - acc: 0.83 - ETA: 47s - loss: 0.3981 - acc: 0.84 - ETA: 47s - loss: 0.3877 - acc: 0.85 - ETA: 47s - loss: 0.4160 - acc: 0.82 - ETA: 46s - loss: 0.4296 - acc: 0.79 - ETA: 46s - loss: 0.4218 - acc: 0.80 - ETA: 46s - loss: 0.4099 - acc: 0.80 - ETA: 45s - loss: 0.4105 - acc: 0.80 - ETA: 45s - loss: 0.4281 - acc: 0.79 - ETA: 45s - loss: 0.4177 - acc: 0.79 - ETA: 45s - loss: 0.4181 - acc: 0.78 - ETA: 45s - loss: 0.4116 - acc: 0.79 - ETA: 44s - loss: 0.4014 - acc: 0.80 - ETA: 44s - loss: 0.3919 - acc: 0.80 - ETA: 44s - loss: 0.3833 - acc: 0.80 - ETA: 44s - loss: 0.4020 - acc: 0.80 - ETA: 43s - loss: 0.4077 - acc: 0.79 - ETA: 43s - loss: 0.3988 - acc: 0.80 - ETA: 43s - loss: 0.3938 - acc: 0.80 - ETA: 43s - loss: 0.3986 - acc: 0.79 - ETA: 42s - loss: 0.3955 - acc: 0.80 - ETA: 42s - loss: 0.3906 - acc: 0.80 - ETA: 42s - loss: 0.4018 - acc: 0.80 - ETA: 42s - loss: 0.3978 - acc: 0.80 - ETA: 41s - loss: 0.3962 - acc: 0.80 - ETA: 41s - loss: 0.3895 - acc: 0.81 - ETA: 41s - loss: 0.3848 - acc: 0.81 - ETA: 41s - loss: 0.3829 - acc: 0.81 - ETA: 40s - loss: 0.3920 - acc: 0.81 - ETA: 40s - loss: 0.3989 - acc: 0.80 - ETA: 40s - loss: 0.4077 - acc: 0.80 - ETA: 40s - loss: 0.4023 - acc: 0.80 - ETA: 39s - loss: 0.3965 - acc: 0.80 - ETA: 39s - loss: 0.3963 - acc: 0.80 - ETA: 39s - loss: 0.3969 - acc: 0.79 - ETA: 39s - loss: 0.3930 - acc: 0.80 - ETA: 38s - loss: 0.3872 - acc: 0.80 - ETA: 38s - loss: 0.3818 - acc: 0.80 - ETA: 38s - loss: 0.3799 - acc: 0.81 - ETA: 38s - loss: 0.3760 - acc: 0.81 - ETA: 37s - loss: 0.3760 - acc: 0.80 - ETA: 37s - loss: 0.3775 - acc: 0.80 - ETA: 37s - loss: 0.3726 - acc: 0.80 - ETA: 37s - loss: 0.3723 - acc: 0.80 - ETA: 36s - loss: 0.3700 - acc: 0.81 - ETA: 36s - loss: 0.3856 - acc: 0.80 - ETA: 36s - loss: 0.3937 - acc: 0.80 - ETA: 36s - loss: 0.3918 - acc: 0.80 - ETA: 35s - loss: 0.4052 - acc: 0.80 - ETA: 35s - loss: 0.4037 - acc: 0.80 - ETA: 35s - loss: 0.4013 - acc: 0.80 - ETA: 35s - loss: 0.3991 - acc: 0.81 - ETA: 34s - loss: 0.3967 - acc: 0.81 - ETA: 34s - loss: 0.3935 - acc: 0.81 - ETA: 34s - loss: 0.3912 - acc: 0.81 - ETA: 34s - loss: 0.3891 - acc: 0.81 - ETA: 33s - loss: 0.3863 - acc: 0.82 - ETA: 33s - loss: 0.3826 - acc: 0.82 - ETA: 33s - loss: 0.3789 - acc: 0.82 - ETA: 32s - loss: 0.3788 - acc: 0.82 - ETA: 32s - loss: 0.3786 - acc: 0.81 - ETA: 32s - loss: 0.3761 - acc: 0.82 - ETA: 32s - loss: 0.3721 - acc: 0.82 - ETA: 31s - loss: 0.3781 - acc: 0.81 - ETA: 31s - loss: 0.3777 - acc: 0.82 - ETA: 31s - loss: 0.3777 - acc: 0.81 - ETA: 31s - loss: 0.3945 - acc: 0.81 - ETA: 30s - loss: 0.4006 - acc: 0.80 - ETA: 30s - loss: 0.3992 - acc: 0.80 - ETA: 30s - loss: 0.3973 - acc: 0.80 - ETA: 30s - loss: 0.3933 - acc: 0.81 - ETA: 29s - loss: 0.3900 - acc: 0.81 - ETA: 29s - loss: 0.3875 - acc: 0.81 - ETA: 29s - loss: 0.3839 - acc: 0.81 - ETA: 29s - loss: 0.3821 - acc: 0.81 - ETA: 28s - loss: 0.3795 - acc: 0.82 - ETA: 28s - loss: 0.3779 - acc: 0.82 - ETA: 28s - loss: 0.3862 - acc: 0.81 - ETA: 28s - loss: 0.3971 - acc: 0.81 - ETA: 27s - loss: 0.3949 - acc: 0.81 - ETA: 27s - loss: 0.3946 - acc: 0.81 - ETA: 27s - loss: 0.3927 - acc: 0.82 - ETA: 27s - loss: 0.3902 - acc: 0.82 - ETA: 26s - loss: 0.3885 - acc: 0.82 - ETA: 26s - loss: 0.3895 - acc: 0.82 - ETA: 26s - loss: 0.3880 - acc: 0.82 - ETA: 26s - loss: 0.3909 - acc: 0.82 - ETA: 25s - loss: 0.3906 - acc: 0.82 - ETA: 25s - loss: 0.3876 - acc: 0.82 - ETA: 25s - loss: 0.3866 - acc: 0.82 - ETA: 25s - loss: 0.3849 - acc: 0.82 - ETA: 24s - loss: 0.3826 - acc: 0.82 - ETA: 24s - loss: 0.3803 - acc: 0.82 - ETA: 24s - loss: 0.3776 - acc: 0.83 - ETA: 24s - loss: 0.3754 - acc: 0.83 - ETA: 23s - loss: 0.3728 - acc: 0.83 - ETA: 23s - loss: 0.3704 - acc: 0.83 - ETA: 23s - loss: 0.3712 - acc: 0.83 - ETA: 23s - loss: 0.3731 - acc: 0.82 - ETA: 22s - loss: 0.3722 - acc: 0.83 - ETA: 22s - loss: 0.3716 - acc: 0.83 - ETA: 22s - loss: 0.3708 - acc: 0.83 - ETA: 22s - loss: 0.3703 - acc: 0.83 - ETA: 21s - loss: 0.3676 - acc: 0.83 - ETA: 21s - loss: 0.3660 - acc: 0.83 - ETA: 21s - loss: 0.3635 - acc: 0.83 - ETA: 20s - loss: 0.3664 - acc: 0.83 - ETA: 20s - loss: 0.3640 - acc: 0.83 - ETA: 20s - loss: 0.3673 - acc: 0.83 - ETA: 20s - loss: 0.3649 - acc: 0.83 - ETA: 19s - loss: 0.3634 - acc: 0.83 - ETA: 19s - loss: 0.3623 - acc: 0.83 - ETA: 19s - loss: 0.3643 - acc: 0.83 - ETA: 19s - loss: 0.3623 - acc: 0.83 - ETA: 18s - loss: 0.3600 - acc: 0.83 - ETA: 18s - loss: 0.3584 - acc: 0.83 - ETA: 18s - loss: 0.3560 - acc: 0.84 - ETA: 18s - loss: 0.3548 - acc: 0.84 - ETA: 17s - loss: 0.3524 - acc: 0.84 - ETA: 17s - loss: 0.3507 - acc: 0.84 - ETA: 17s - loss: 0.3536 - acc: 0.84 - ETA: 17s - loss: 0.3532 - acc: 0.84 - ETA: 16s - loss: 0.3593 - acc: 0.84 - ETA: 16s - loss: 0.3615 - acc: 0.83 - ETA: 16s - loss: 0.3594 - acc: 0.83 - ETA: 16s - loss: 0.3580 - acc: 0.83 - ETA: 15s - loss: 0.3561 - acc: 0.84 - ETA: 15s - loss: 0.3549 - acc: 0.84 - ETA: 15s - loss: 0.3545 - acc: 0.84 - ETA: 15s - loss: 0.3524 - acc: 0.84 - ETA: 14s - loss: 0.3531 - acc: 0.84 - ETA: 14s - loss: 0.3510 - acc: 0.84 - ETA: 14s - loss: 0.3494 - acc: 0.84 - ETA: 14s - loss: 0.3559 - acc: 0.84 - ETA: 13s - loss: 0.3612 - acc: 0.83 - ETA: 13s - loss: 0.3590 - acc: 0.84 - ETA: 13s - loss: 0.3574 - acc: 0.84 - ETA: 13s - loss: 0.3557 - acc: 0.84 - ETA: 12s - loss: 0.3536 - acc: 0.84 - ETA: 12s - loss: 0.3536 - acc: 0.84 - ETA: 12s - loss: 0.3516 - acc: 0.84 - ETA: 12s - loss: 0.3504 - acc: 0.84 - ETA: 11s - loss: 0.3490 - acc: 0.84 - ETA: 11s - loss: 0.3473 - acc: 0.84 - ETA: 11s - loss: 0.3453 - acc: 0.84 - ETA: 11s - loss: 0.3444 - acc: 0.84 - ETA: 10s - loss: 0.3426 - acc: 0.85 - ETA: 10s - loss: 0.3407 - acc: 0.85 - ETA: 10s - loss: 0.3397 - acc: 0.85 - ETA: 9s - loss: 0.3380 - acc: 0.8531 - ETA: 9s - loss: 0.3396 - acc: 0.851 - ETA: 9s - loss: 0.3377 - acc: 0.852 - ETA: 9s - loss: 0.3381 - acc: 0.850 - ETA: 8s - loss: 0.3367 - acc: 0.850 - ETA: 8s - loss: 0.3372 - acc: 0.848 - ETA: 8s - loss: 0.3355 - acc: 0.849 - ETA: 8s - loss: 0.3337 - acc: 0.850 - ETA: 7s - loss: 0.3351 - acc: 0.848 - ETA: 7s - loss: 0.3336 - acc: 0.849 - ETA: 7s - loss: 0.3355 - acc: 0.847 - ETA: 7s - loss: 0.3339 - acc: 0.848 - ETA: 6s - loss: 0.3347 - acc: 0.846 - ETA: 6s - loss: 0.3337 - acc: 0.847 - ETA: 6s - loss: 0.3319 - acc: 0.848 - ETA: 6s - loss: 0.3314 - acc: 0.849 - ETA: 5s - loss: 0.3301 - acc: 0.849 - ETA: 5s - loss: 0.3285 - acc: 0.850 - ETA: 5s - loss: 0.3269 - acc: 0.851 - ETA: 5s - loss: 0.3263 - acc: 0.852 - ETA: 4s - loss: 0.3302 - acc: 0.847 - ETA: 4s - loss: 0.3407 - acc: 0.843 - ETA: 4s - loss: 0.3390 - acc: 0.844 - ETA: 4s - loss: 0.3377 - acc: 0.845 - ETA: 3s - loss: 0.3421 - acc: 0.843 - ETA: 3s - loss: 0.3421 - acc: 0.844 - ETA: 3s - loss: 0.3433 - acc: 0.842 - ETA: 3s - loss: 0.3457 - acc: 0.840 - ETA: 2s - loss: 0.3442 - acc: 0.841 - ETA: 2s - loss: 0.3430 - acc: 0.842 - ETA: 2s - loss: 0.3420 - acc: 0.843 - ETA: 2s - loss: 0.3409 - acc: 0.843 - ETA: 1s - loss: 0.3393 - acc: 0.844 - ETA: 1s - loss: 0.3384 - acc: 0.845 - ETA: 1s - loss: 0.3369 - acc: 0.846 - ETA: 1s - loss: 0.3385 - acc: 0.844 - ETA: 0s - loss: 0.3388 - acc: 0.842 - ETA: 0s - loss: 0.3390 - acc: 0.843 - ETA: 0s - loss: 0.3397 - acc: 0.8419"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 61s 142ms/step - loss: 0.3446 - acc: 0.8403 - val_loss: 0.8244 - val_acc: 0.6458\n",
      "120/120 [==============================] - ETA: 22 - ETA: 18 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 15s 126ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-10 08:56:15,007] A new study created in memory with name: CatsAndDogs_Simple\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-10 08:56:15.011475] [OptKeras] Ready for optimization. (message printed as verbose is set to 1+)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-260416cbadca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cp-{epoch:04d}.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mcheckpoint_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "from optkeras.optkeras import OptKeras\n",
    "import optkeras\n",
    "import pickle\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input,Dense,BatchNormalization,Flatten,Dropout,GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import layer_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model,load_model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import optuna\n",
    "STUDY = None\n",
    "N_TRIALS = 3\n",
    "EPOCHS = 3\n",
    "\n",
    "optuna_csv = 'optuna.csv'\n",
    "\n",
    "with open('training.pkl', 'rb') as f:\n",
    "     train = pickle.load(f)\n",
    "    \n",
    "with open('testing.pkl', 'rb') as f:\n",
    "     test = pickle.load(f)\n",
    "        \n",
    "with open('validation.pkl','rb') as f:\n",
    "     val = pickle.load(f)\n",
    "        \n",
    "train_photos, train_labels = list(), list()\n",
    "tp = list()\n",
    "for file in train:\n",
    "    if 'Cat' in file:\n",
    "        output = 1.0\n",
    "    else:\n",
    "        output = 0.0\n",
    "    photo = load_img(file)\n",
    "    photo = img_to_array(photo)\n",
    "    train_photos.append(photo)\n",
    "    train_labels.append(output)\n",
    "train_photos = asarray(train_photos)\n",
    "train_labels = asarray(train_labels)\n",
    "\n",
    "test_photos, test_labels = list(), list()\n",
    "for file in test:\n",
    "    if 'Cat' in file:\n",
    "        output = 1.0\n",
    "    else:\n",
    "        output = 0.0\n",
    "    photo = load_img(file)\n",
    "    photo = img_to_array(photo)\n",
    "    tp.append(photo)\n",
    "    test_photos.append(photo)\n",
    "    test_labels.append(output)\n",
    "test_photos = asarray(test_photos)\n",
    "test_labels = asarray(test_labels)\n",
    "\n",
    "val_photos, val_labels = list(), list()\n",
    "for file in val:\n",
    "    if 'Cat' in file:\n",
    "        output = 1.0\n",
    "    else:\n",
    "        output = 0.0\n",
    "    photo = load_img(file)\n",
    "    photo = img_to_array(photo)\n",
    "    val_photos.append(photo)\n",
    "    val_labels.append(output)\n",
    "val_photos = asarray(val_photos)\n",
    "val_labels = asarray(val_labels)\n",
    "\n",
    "train_photos =train_photos.astype('float32')\n",
    "test_photos= test_photos.astype('float32')\n",
    "val_photos =val_photos.astype('float32')\n",
    "train_photos /= 255\n",
    "val_photos /= 255\n",
    "test_photos /= 255\n",
    "\n",
    "\n",
    "nb_classes = 2\n",
    "epochs=3\n",
    "batch_size =2\n",
    "\n",
    "vgg16_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "x = vgg16_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(nb_classes, activation = 'softmax')(x)\n",
    "model = Model(input = vgg16_model.input, output = predictions)\n",
    "\n",
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer = 'rmsprop',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model_info = model.fit(x=train_photos, y=train_labels,batch_size=2 , epochs=epochs, \n",
    "                   verbose=1,validation_data=(val_photos,val_labels))\n",
    "\n",
    "model.save('model_hpo.h5')\n",
    "\n",
    "m= load_model(\"model_hpo.h5\")\n",
    "test_photos = test_photos.astype(\"float32\")\n",
    "\n",
    "results = m.evaluate(test_photos, test_labels, batch_size=1)\n",
    "optkeras.optkeras.get_trial_default = lambda: optuna.trial.FrozenTrial(\n",
    "            None, None, None, None, None, None, None, None, None, None, None)\n",
    "study_name = \"CatsAndDogs\" + '_Simple'\n",
    "ok = OptKeras(study_name=study_name,\n",
    "              monitor='val_acc',\n",
    "              direction='maximize')\n",
    "\n",
    "checkpoint_path = \"cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "def objective(trial):\n",
    "    nb_classes = 2\n",
    "    epochs=3\n",
    "    batch_size =2\n",
    "    optimizer_options = [\"RMSprop\", \"Adam\", \"SGD\"]\n",
    "\n",
    "    vgg16_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "    x = vgg16_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation=trial.suggest_categorical('activation', ['relu', 'linear']))(x)\n",
    "    predictions = Dense(nb_classes, activation = 'softmax')(x)\n",
    "    model = Model(input = vgg16_model.input, output = predictions)\n",
    "    for layer in vgg16_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer = trial.suggest_categorical(\"optimizer\", [\"rmsprop\", \"Adam\", \"SGD\"]),loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    period=5)\n",
    "    model.save_weights(checkpoint_path.format(epoch=0))\n",
    "    model.fit(x=train_photos, y=train_labels,batch_size=2 , epochs=epochs, callbacks=ok.callbacks(trial),\n",
    "              verbose=ok.keras_verbose ,validation_data=(val_photos,val_labels))\n",
    "    return ok.trial_best_value\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import signal\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "CHECKPOINT_FILE = \"saved_stated.txt\"\n",
    "\n",
    "i = 0\n",
    "try:\n",
    "    with open(CHECKPOINT_FILE, mode=\"r\") as f:\n",
    "        i = int(f.read())\n",
    "            \n",
    "    def SIGTERM_handler(signum, frame):\n",
    "        with open(CHECKPOINT_FILE, mode=\"w\") as f:\n",
    "            f.write(str(i))\n",
    "            \n",
    "    signal.signal(signal.SIGTERM, SIGTERM_handler)\n",
    "    for _ in range(i,N_TRIALS+1):\n",
    "        ok.optimize(objective, timeout = 3*60)\n",
    "        if i == N_TRIALS:\n",
    "            break\n",
    "        i += 1\n",
    "        time.sleep(1)\n",
    "        with open(CHECKPOINT_FILE, mode=\"w\") as f:\n",
    "            f.write(str(i))\n",
    "            \n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "\n",
    "        \n",
    "# ok.optimize(objective, timeout = 3*60)\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_rows = 8 \n",
    "\n",
    "pd.options.display.max_rows = 8\n",
    "\n",
    "put_csv = pd.read_csv(ok.keras_log_file_path)\n",
    "put_csv.to_csv(optuna_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "STUDY = joblib.load(\"study_checkpoint_\" + \"vgg\" + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_activation</th>\n",
       "      <th>params_optimizer</th>\n",
       "      <th>user_attrs__Datetime_epoch_begin</th>\n",
       "      <th>user_attrs__Datetime_epoch_end</th>\n",
       "      <th>user_attrs__Trial_num</th>\n",
       "      <th>user_attrs_acc</th>\n",
       "      <th>user_attrs_loss</th>\n",
       "      <th>user_attrs_val_acc</th>\n",
       "      <th>user_attrs_val_loss</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>2020-09-10 10:38:04.364084</td>\n",
       "      <td>2020-09-10 10:41:17.425286</td>\n",
       "      <td>0 days 00:03:13.061202</td>\n",
       "      <td>linear</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>2020-09-10 10:40:10.220480</td>\n",
       "      <td>2020-09-10 10:41:17.036755</td>\n",
       "      <td>0</td>\n",
       "      <td>0.840278</td>\n",
       "      <td>0.371861</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.281831</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number  value             datetime_start          datetime_complete  \\\n",
       "0       0  0.875 2020-09-10 10:38:04.364084 2020-09-10 10:41:17.425286   \n",
       "\n",
       "                duration params_activation params_optimizer  \\\n",
       "0 0 days 00:03:13.061202            linear          rmsprop   \n",
       "\n",
       "  user_attrs__Datetime_epoch_begin user_attrs__Datetime_epoch_end  \\\n",
       "0       2020-09-10 10:40:10.220480     2020-09-10 10:41:17.036755   \n",
       "\n",
       "   user_attrs__Trial_num  user_attrs_acc  user_attrs_loss  user_attrs_val_acc  \\\n",
       "0                      0        0.840278         0.371861               0.875   \n",
       "\n",
       "   user_attrs_val_loss     state  \n",
       "0             0.281831  COMPLETE  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STUDY.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_activation</th>\n",
       "      <th>params_optimizer</th>\n",
       "      <th>user_attrs__Datetime_epoch_begin</th>\n",
       "      <th>user_attrs__Datetime_epoch_end</th>\n",
       "      <th>user_attrs__Trial_num</th>\n",
       "      <th>user_attrs_acc</th>\n",
       "      <th>user_attrs_loss</th>\n",
       "      <th>user_attrs_val_acc</th>\n",
       "      <th>user_attrs_val_loss</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>2020-09-10 09:25:21.972578</td>\n",
       "      <td>2020-09-10 09:28:35.769124</td>\n",
       "      <td>0 days 00:03:13.796546</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam</td>\n",
       "      <td>2020-09-10 09:27:29.188768</td>\n",
       "      <td>2020-09-10 09:28:35.342965</td>\n",
       "      <td>0</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.258454</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.206545</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>2020-09-10 09:28:35.779395</td>\n",
       "      <td>2020-09-10 09:31:50.288052</td>\n",
       "      <td>0 days 00:03:14.508657</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam</td>\n",
       "      <td>2020-09-10 09:30:48.136385</td>\n",
       "      <td>2020-09-10 09:31:50.286750</td>\n",
       "      <td>1</td>\n",
       "      <td>0.865741</td>\n",
       "      <td>0.313100</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.497141</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>2020-09-10 09:31:50.298797</td>\n",
       "      <td>2020-09-10 09:35:08.326404</td>\n",
       "      <td>0 days 00:03:18.027607</td>\n",
       "      <td>relu</td>\n",
       "      <td>SGD</td>\n",
       "      <td>2020-09-10 09:34:03.355682</td>\n",
       "      <td>2020-09-10 09:35:07.994272</td>\n",
       "      <td>2</td>\n",
       "      <td>0.715278</td>\n",
       "      <td>0.584149</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.539775</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2020-09-10 09:35:08.338973</td>\n",
       "      <td>2020-09-10 09:38:22.295187</td>\n",
       "      <td>0 days 00:03:13.956214</td>\n",
       "      <td>relu</td>\n",
       "      <td>SGD</td>\n",
       "      <td>2020-09-10 09:37:16.917617</td>\n",
       "      <td>2020-09-10 09:38:22.293684</td>\n",
       "      <td>3</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.597572</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>2020-09-10 09:38:22.438172</td>\n",
       "      <td>2020-09-10 09:41:52.043182</td>\n",
       "      <td>0 days 00:03:29.605010</td>\n",
       "      <td>relu</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>2020-09-10 09:40:45.961300</td>\n",
       "      <td>2020-09-10 09:41:51.558353</td>\n",
       "      <td>4</td>\n",
       "      <td>0.844907</td>\n",
       "      <td>0.334146</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.270747</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>2020-09-10 10:19:11.161222</td>\n",
       "      <td>2020-09-10 10:22:25.163843</td>\n",
       "      <td>0 days 00:03:14.002621</td>\n",
       "      <td>relu</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>2020-09-10 10:21:18.473568</td>\n",
       "      <td>2020-09-10 10:22:25.162490</td>\n",
       "      <td>5</td>\n",
       "      <td>0.842593</td>\n",
       "      <td>0.330920</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.479238</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number     value             datetime_start          datetime_complete  \\\n",
       "0       0  0.937500 2020-09-10 09:25:21.972578 2020-09-10 09:28:35.769124   \n",
       "1       1  0.708333 2020-09-10 09:28:35.779395 2020-09-10 09:31:50.288052   \n",
       "2       2  0.770833 2020-09-10 09:31:50.298797 2020-09-10 09:35:08.326404   \n",
       "3       3  0.666667 2020-09-10 09:35:08.338973 2020-09-10 09:38:22.295187   \n",
       "4       4  0.895833 2020-09-10 09:38:22.438172 2020-09-10 09:41:52.043182   \n",
       "5       5  0.770833 2020-09-10 10:19:11.161222 2020-09-10 10:22:25.163843   \n",
       "\n",
       "                duration params_activation params_optimizer  \\\n",
       "0 0 days 00:03:13.796546              relu             Adam   \n",
       "1 0 days 00:03:14.508657              relu             Adam   \n",
       "2 0 days 00:03:18.027607              relu              SGD   \n",
       "3 0 days 00:03:13.956214              relu              SGD   \n",
       "4 0 days 00:03:29.605010              relu          rmsprop   \n",
       "5 0 days 00:03:14.002621              relu          rmsprop   \n",
       "\n",
       "  user_attrs__Datetime_epoch_begin user_attrs__Datetime_epoch_end  \\\n",
       "0       2020-09-10 09:27:29.188768     2020-09-10 09:28:35.342965   \n",
       "1       2020-09-10 09:30:48.136385     2020-09-10 09:31:50.286750   \n",
       "2       2020-09-10 09:34:03.355682     2020-09-10 09:35:07.994272   \n",
       "3       2020-09-10 09:37:16.917617     2020-09-10 09:38:22.293684   \n",
       "4       2020-09-10 09:40:45.961300     2020-09-10 09:41:51.558353   \n",
       "5       2020-09-10 10:21:18.473568     2020-09-10 10:22:25.162490   \n",
       "\n",
       "   user_attrs__Trial_num  user_attrs_acc  user_attrs_loss  user_attrs_val_acc  \\\n",
       "0                      0        0.902778         0.258454            0.937500   \n",
       "1                      1        0.865741         0.313100            0.708333   \n",
       "2                      2        0.715278         0.584149            0.770833   \n",
       "3                      3        0.687500         0.597572            0.666667   \n",
       "4                      4        0.844907         0.334146            0.895833   \n",
       "5                      5        0.842593         0.330920            0.770833   \n",
       "\n",
       "   user_attrs_val_loss     state  \n",
       "0             0.206545  COMPLETE  \n",
       "1             0.497141  COMPLETE  \n",
       "2             0.539775  COMPLETE  \n",
       "3             0.572754  COMPLETE  \n",
       "4             0.270747  COMPLETE  \n",
       "5             0.479238  COMPLETE  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "new1 = joblib.load(\"hpo_study_checkpoint1_\" + \"vgg\" + \".pkl\")\n",
    "new1.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input,Dense,BatchNormalization,Flatten,Dropout,GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import layer_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model,load_model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "N_TRAIN_EXAMPLES = len(train_photos)\n",
    "N_VALID_EXAMPLES = len(val_photos)\n",
    "BATCHSIZE = 1\n",
    "CLASSES = 2\n",
    "EPOCHS = 3\n",
    "STUDY = None\n",
    "N_TRIALS = 10\n",
    "TAR_FILE = \"basicnet_model.tar.gz\"\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "#     clear_session()\n",
    "\n",
    "    (x_train, y_train), (x_valid, y_valid) = (train_photos,train_labels),(val_photos,val_labels)\n",
    "#     img_x, img_y = x_train.shape[1], x_train.shape[2]\n",
    "#     x_train = x_train.reshape(-1, img_x, img_y, 1)[:N_TRAIN_EXAMPLES].astype(\"float32\") / 255\n",
    "#     x_valid = x_valid.reshape(-1, img_x, img_y, 1)[:N_VALID_EXAMPLES].astype(\"float32\") / 255\n",
    "#     y_train = y_train[:N_TRAIN_EXAMPLES]\n",
    "#     y_valid = y_valid[:N_VALID_EXAMPLES]\n",
    "#     input_shape = (img_x, img_y, 1)\n",
    "    vgg16_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "    x = vgg16_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(CLASSES, activation = 'softmax')(x)\n",
    "    model = Model(inputs = vgg16_model.input, outputs = predictions)\n",
    "    \n",
    "    for layer in vgg16_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # We compile our model with a sampled learning rate.\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=RMSprop(lr=lr), metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        shuffle=True,\n",
    "        batch_size=BATCHSIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "    return score[1]\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "def create_classifier(trial):\n",
    "    \n",
    "    # We optimize the numbers of layers and their units.\n",
    "    vgg16_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "    x = vgg16_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(CLASSES, activation = 'softmax')(x)\n",
    "    model = Model(inputs = vgg16_model.input, outputs = predictions)\n",
    "    \n",
    "    for layer in vgg16_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=RMSprop(lr=lr), metrics=[\"accuracy\"])\n",
    "    my_callbacks = [tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')]\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        shuffle=True,\n",
    "        batch_size=BATCHSIZE,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=my_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "    return score[1]\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    classifier = create_classifier(trial)\n",
    "\n",
    "    classifier.train(input_fn=train_input_fn, steps=TRAIN_STEPS)\n",
    "\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "    return float(eval_results[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-10 08:57:26,532] A new study created in memory with name: no-name-3ac9f081-8f85-4de9-9f2a-b71f869e6ae1\n",
      "[W 2020-09-10 08:57:27,252] Trial 0 failed because of the following error: NameError(\"name 'tf' is not defined\",)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/optuna/study.py\", line 778, in _run_trial\n",
      "    result = func(trial)\n",
      "  File \"<ipython-input-6-cb39e55214f5>\", line 112, in objective\n",
      "    classifier = create_classifier(trial)\n",
      "  File \"<ipython-input-6-cb39e55214f5>\", line 91, in create_classifier\n",
      "    my_callbacks = [tf.keras.callbacks.EarlyStopping(patience=2),\n",
      "NameError: name 'tf' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-94d5ab7067a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 self._optimize_sequential(\n\u001b[0;32m--> 328\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m                 )\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    724\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    753\u001b[0m     ) -> None:\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Trial {} pruned. {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-cb39e55214f5>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-cb39e55214f5>\u001b[0m in \u001b[0;36mcreate_classifier\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     89\u001b[0m     model.compile(\n\u001b[1;32m     90\u001b[0m         loss=\"sparse_categorical_crossentropy\", optimizer=RMSprop(lr=lr), metrics=[\"accuracy\"])\n\u001b[0;32m---> 91\u001b[0;31m     my_callbacks = [tf.keras.callbacks.EarlyStopping(patience=2),\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.{epoch:02d}-{val_loss:.2f}.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     tf.keras.callbacks.TensorBoard(log_dir='./logs')]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "study = optuna.study.create_study( direction='maximize')\n",
    "study.optimize(objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
