{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "import sys\n",
    "import pickle\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input,Dense,BatchNormalization,Flatten,Dropout,GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import layer_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model,load_model\n",
    "\n",
    "optuna_csv = sys.argv[1]\n",
    "\n",
    "with open('training.pkl', 'rb') as f:\n",
    "     train = pickle.load(f)\n",
    "    \n",
    "with open('testing.pkl', 'rb') as f:\n",
    "     test = pickle.load(f)\n",
    "        \n",
    "with open('validation.pkl','rb') as f:\n",
    "     val = pickle.load(f)\n",
    "        \n",
    "train_photos, train_labels = list(), list()\n",
    "tp = list()\n",
    "for file in train:\n",
    "    if 'Cat' in file:\n",
    "        output = 1.0\n",
    "    else:\n",
    "        output = 0.0\n",
    "    photo = load_img(file)\n",
    "    photo = img_to_array(photo)\n",
    "    train_photos.append(photo)\n",
    "    train_labels.append(output)\n",
    "train_photos = asarray(train_photos)\n",
    "train_labels = asarray(train_labels)\n",
    "\n",
    "test_photos, test_labels = list(), list()\n",
    "for file in test:\n",
    "    if 'Cat' in file:\n",
    "        output = 1.0\n",
    "    else:\n",
    "        output = 0.0\n",
    "    photo = load_img(file)\n",
    "    photo = img_to_array(photo)\n",
    "    tp.append(photo)\n",
    "    test_photos.append(photo)\n",
    "    test_labels.append(output)\n",
    "test_photos = asarray(test_photos)\n",
    "test_labels = asarray(test_labels)\n",
    "\n",
    "val_photos, val_labels = list(), list()\n",
    "for file in val:\n",
    "    if 'Cat' in file:\n",
    "        output = 1.0\n",
    "    else:\n",
    "        output = 0.0\n",
    "    photo = load_img(file)\n",
    "    photo = img_to_array(photo)\n",
    "    val_photos.append(photo)\n",
    "    val_labels.append(output)\n",
    "val_photos = asarray(val_photos)\n",
    "val_labels = asarray(val_labels)\n",
    "\n",
    "train_photos =train_photos.astype('float32')\n",
    "test_photos= test_photos.astype('float32')\n",
    "val_photos =val_photos.astype('float32')\n",
    "train_photos /= 255\n",
    "val_photos /= 255\n",
    "test_photos /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 432 samples, validate on 48 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/432 [============================>.] - ETA: 1:28 - loss: 0.7249 - acc: 0.0000e+0 - ETA: 1:11 - loss: 2.3215 - acc: 0.2500    - ETA: 1:06 - loss: 1.5594 - acc: 0.500 - ETA: 1:03 - loss: 1.6531 - acc: 0.500 - ETA: 1:01 - loss: 1.4624 - acc: 0.500 - ETA: 59s - loss: 1.8200 - acc: 0.416 - ETA: 58s - loss: 1.6073 - acc: 0.50 - ETA: 57s - loss: 1.4133 - acc: 0.56 - ETA: 56s - loss: 1.2595 - acc: 0.61 - ETA: 55s - loss: 1.1372 - acc: 0.65 - ETA: 55s - loss: 1.3700 - acc: 0.59 - ETA: 54s - loss: 1.3276 - acc: 0.58 - ETA: 54s - loss: 1.2774 - acc: 0.57 - ETA: 53s - loss: 1.2554 - acc: 0.57 - ETA: 53s - loss: 1.2320 - acc: 0.53 - ETA: 53s - loss: 1.1616 - acc: 0.56 - ETA: 52s - loss: 1.1679 - acc: 0.55 - ETA: 52s - loss: 1.1439 - acc: 0.55 - ETA: 51s - loss: 1.0921 - acc: 0.57 - ETA: 51s - loss: 1.0408 - acc: 0.60 - ETA: 51s - loss: 0.9939 - acc: 0.61 - ETA: 51s - loss: 1.0218 - acc: 0.61 - ETA: 50s - loss: 1.0368 - acc: 0.58 - ETA: 50s - loss: 1.0302 - acc: 0.58 - ETA: 50s - loss: 1.0116 - acc: 0.60 - ETA: 49s - loss: 1.0021 - acc: 0.57 - ETA: 49s - loss: 0.9998 - acc: 0.57 - ETA: 49s - loss: 0.9803 - acc: 0.58 - ETA: 49s - loss: 0.9682 - acc: 0.58 - ETA: 48s - loss: 0.9637 - acc: 0.56 - ETA: 48s - loss: 0.9471 - acc: 0.58 - ETA: 48s - loss: 0.9398 - acc: 0.56 - ETA: 48s - loss: 0.9590 - acc: 0.54 - ETA: 47s - loss: 0.9663 - acc: 0.52 - ETA: 47s - loss: 0.9631 - acc: 0.52 - ETA: 47s - loss: 0.9474 - acc: 0.54 - ETA: 46s - loss: 0.9261 - acc: 0.55 - ETA: 46s - loss: 0.9765 - acc: 0.53 - ETA: 46s - loss: 0.9868 - acc: 0.52 - ETA: 46s - loss: 1.0218 - acc: 0.51 - ETA: 45s - loss: 1.0149 - acc: 0.51 - ETA: 45s - loss: 1.0022 - acc: 0.52 - ETA: 45s - loss: 0.9944 - acc: 0.52 - ETA: 44s - loss: 0.9939 - acc: 0.52 - ETA: 44s - loss: 0.9938 - acc: 0.51 - ETA: 44s - loss: 0.9893 - acc: 0.51 - ETA: 44s - loss: 0.9753 - acc: 0.52 - ETA: 44s - loss: 0.9817 - acc: 0.51 - ETA: 43s - loss: 0.9785 - acc: 0.51 - ETA: 43s - loss: 0.9912 - acc: 0.50 - ETA: 43s - loss: 0.9852 - acc: 0.50 - ETA: 43s - loss: 0.9741 - acc: 0.50 - ETA: 43s - loss: 0.9694 - acc: 0.50 - ETA: 42s - loss: 0.9638 - acc: 0.50 - ETA: 42s - loss: 0.9562 - acc: 0.51 - ETA: 42s - loss: 0.9505 - acc: 0.52 - ETA: 42s - loss: 0.9643 - acc: 0.51 - ETA: 41s - loss: 0.9540 - acc: 0.52 - ETA: 41s - loss: 0.9393 - acc: 0.53 - ETA: 41s - loss: 0.9602 - acc: 0.52 - ETA: 41s - loss: 0.9522 - acc: 0.52 - ETA: 40s - loss: 0.9454 - acc: 0.53 - ETA: 40s - loss: 0.9407 - acc: 0.53 - ETA: 40s - loss: 0.9474 - acc: 0.53 - ETA: 40s - loss: 0.9420 - acc: 0.53 - ETA: 39s - loss: 0.9359 - acc: 0.53 - ETA: 39s - loss: 0.9360 - acc: 0.53 - ETA: 39s - loss: 0.9302 - acc: 0.53 - ETA: 39s - loss: 0.9262 - acc: 0.53 - ETA: 38s - loss: 0.9226 - acc: 0.53 - ETA: 38s - loss: 0.9188 - acc: 0.53 - ETA: 38s - loss: 0.9132 - acc: 0.54 - ETA: 37s - loss: 0.9055 - acc: 0.54 - ETA: 37s - loss: 0.8989 - acc: 0.55 - ETA: 37s - loss: 0.8970 - acc: 0.55 - ETA: 37s - loss: 0.8952 - acc: 0.55 - ETA: 36s - loss: 0.8893 - acc: 0.55 - ETA: 36s - loss: 0.8957 - acc: 0.55 - ETA: 36s - loss: 0.8873 - acc: 0.55 - ETA: 36s - loss: 0.8836 - acc: 0.55 - ETA: 35s - loss: 0.8748 - acc: 0.56 - ETA: 35s - loss: 0.8652 - acc: 0.56 - ETA: 35s - loss: 0.8645 - acc: 0.56 - ETA: 35s - loss: 0.8622 - acc: 0.57 - ETA: 34s - loss: 0.8580 - acc: 0.57 - ETA: 34s - loss: 0.8535 - acc: 0.58 - ETA: 34s - loss: 0.8466 - acc: 0.58 - ETA: 34s - loss: 0.8401 - acc: 0.59 - ETA: 33s - loss: 0.8318 - acc: 0.59 - ETA: 33s - loss: 0.8343 - acc: 0.58 - ETA: 33s - loss: 0.8259 - acc: 0.59 - ETA: 33s - loss: 0.8236 - acc: 0.59 - ETA: 32s - loss: 0.8178 - acc: 0.59 - ETA: 32s - loss: 0.8130 - acc: 0.60 - ETA: 32s - loss: 0.8094 - acc: 0.60 - ETA: 32s - loss: 0.8021 - acc: 0.60 - ETA: 31s - loss: 0.8072 - acc: 0.60 - ETA: 31s - loss: 0.8128 - acc: 0.60 - ETA: 31s - loss: 0.8086 - acc: 0.60 - ETA: 30s - loss: 0.8087 - acc: 0.60 - ETA: 30s - loss: 0.8048 - acc: 0.60 - ETA: 30s - loss: 0.8024 - acc: 0.60 - ETA: 30s - loss: 0.7995 - acc: 0.60 - ETA: 29s - loss: 0.7967 - acc: 0.60 - ETA: 29s - loss: 0.7918 - acc: 0.60 - ETA: 29s - loss: 0.7908 - acc: 0.60 - ETA: 29s - loss: 0.8024 - acc: 0.59 - ETA: 28s - loss: 0.7992 - acc: 0.59 - ETA: 28s - loss: 0.7954 - acc: 0.60 - ETA: 28s - loss: 0.7893 - acc: 0.60 - ETA: 28s - loss: 0.7903 - acc: 0.60 - ETA: 27s - loss: 0.7965 - acc: 0.60 - ETA: 27s - loss: 0.8001 - acc: 0.59 - ETA: 27s - loss: 0.7936 - acc: 0.60 - ETA: 26s - loss: 0.7890 - acc: 0.60 - ETA: 26s - loss: 0.7861 - acc: 0.60 - ETA: 26s - loss: 0.7864 - acc: 0.60 - ETA: 26s - loss: 0.7883 - acc: 0.60 - ETA: 25s - loss: 0.7864 - acc: 0.60 - ETA: 25s - loss: 0.7874 - acc: 0.60 - ETA: 25s - loss: 0.7846 - acc: 0.61 - ETA: 25s - loss: 0.7885 - acc: 0.61 - ETA: 24s - loss: 0.7859 - acc: 0.61 - ETA: 24s - loss: 0.7888 - acc: 0.61 - ETA: 24s - loss: 0.7878 - acc: 0.61 - ETA: 24s - loss: 0.7847 - acc: 0.61 - ETA: 23s - loss: 0.7825 - acc: 0.61 - ETA: 23s - loss: 0.7775 - acc: 0.61 - ETA: 23s - loss: 0.7747 - acc: 0.62 - ETA: 22s - loss: 0.7792 - acc: 0.61 - ETA: 22s - loss: 0.7775 - acc: 0.61 - ETA: 22s - loss: 0.7755 - acc: 0.61 - ETA: 22s - loss: 0.7764 - acc: 0.61 - ETA: 21s - loss: 0.7741 - acc: 0.61 - ETA: 21s - loss: 0.7690 - acc: 0.61 - ETA: 21s - loss: 0.7639 - acc: 0.62 - ETA: 21s - loss: 0.7587 - acc: 0.62 - ETA: 20s - loss: 0.7535 - acc: 0.62 - ETA: 20s - loss: 0.7557 - acc: 0.62 - ETA: 20s - loss: 0.7519 - acc: 0.62 - ETA: 20s - loss: 0.7497 - acc: 0.62 - ETA: 19s - loss: 0.7513 - acc: 0.62 - ETA: 19s - loss: 0.7466 - acc: 0.62 - ETA: 19s - loss: 0.7414 - acc: 0.63 - ETA: 18s - loss: 0.7429 - acc: 0.63 - ETA: 18s - loss: 0.7418 - acc: 0.63 - ETA: 18s - loss: 0.7391 - acc: 0.63 - ETA: 18s - loss: 0.7370 - acc: 0.63 - ETA: 17s - loss: 0.7376 - acc: 0.63 - ETA: 17s - loss: 0.7348 - acc: 0.63 - ETA: 17s - loss: 0.7322 - acc: 0.63 - ETA: 17s - loss: 0.7295 - acc: 0.64 - ETA: 16s - loss: 0.7270 - acc: 0.64 - ETA: 16s - loss: 0.7238 - acc: 0.64 - ETA: 16s - loss: 0.7214 - acc: 0.64 - ETA: 16s - loss: 0.7194 - acc: 0.64 - ETA: 15s - loss: 0.7190 - acc: 0.64 - ETA: 15s - loss: 0.7159 - acc: 0.64 - ETA: 15s - loss: 0.7123 - acc: 0.65 - ETA: 14s - loss: 0.7100 - acc: 0.65 - ETA: 14s - loss: 0.7058 - acc: 0.65 - ETA: 14s - loss: 0.7069 - acc: 0.65 - ETA: 14s - loss: 0.7039 - acc: 0.65 - ETA: 13s - loss: 0.7066 - acc: 0.65 - ETA: 13s - loss: 0.7026 - acc: 0.65 - ETA: 13s - loss: 0.6985 - acc: 0.65 - ETA: 13s - loss: 0.7022 - acc: 0.65 - ETA: 12s - loss: 0.7015 - acc: 0.65 - ETA: 12s - loss: 0.6988 - acc: 0.65 - ETA: 12s - loss: 0.6948 - acc: 0.66 - ETA: 12s - loss: 0.7078 - acc: 0.65 - ETA: 11s - loss: 0.7057 - acc: 0.65 - ETA: 11s - loss: 0.7090 - acc: 0.65 - ETA: 11s - loss: 0.7066 - acc: 0.65 - ETA: 11s - loss: 0.7051 - acc: 0.65 - ETA: 10s - loss: 0.7054 - acc: 0.65 - ETA: 10s - loss: 0.7030 - acc: 0.65 - ETA: 10s - loss: 0.7003 - acc: 0.66 - ETA: 9s - loss: 0.7003 - acc: 0.6592 - ETA: 9s - loss: 0.7007 - acc: 0.658 - ETA: 9s - loss: 0.6972 - acc: 0.660 - ETA: 9s - loss: 0.6972 - acc: 0.659 - ETA: 8s - loss: 0.6948 - acc: 0.661 - ETA: 8s - loss: 0.6921 - acc: 0.663 - ETA: 8s - loss: 0.6885 - acc: 0.664 - ETA: 8s - loss: 0.6885 - acc: 0.664 - ETA: 7s - loss: 0.6900 - acc: 0.660 - ETA: 7s - loss: 0.6872 - acc: 0.662 - ETA: 7s - loss: 0.6876 - acc: 0.661 - ETA: 7s - loss: 0.6843 - acc: 0.663 - ETA: 6s - loss: 0.6872 - acc: 0.659 - ETA: 6s - loss: 0.6882 - acc: 0.658 - ETA: 6s - loss: 0.6862 - acc: 0.660 - ETA: 5s - loss: 0.6841 - acc: 0.662 - ETA: 5s - loss: 0.6836 - acc: 0.661 - ETA: 5s - loss: 0.6832 - acc: 0.663 - ETA: 5s - loss: 0.6825 - acc: 0.662 - ETA: 4s - loss: 0.6847 - acc: 0.661 - ETA: 4s - loss: 0.6843 - acc: 0.660 - ETA: 4s - loss: 0.6850 - acc: 0.660 - ETA: 4s - loss: 0.6834 - acc: 0.661 - ETA: 3s - loss: 0.6840 - acc: 0.660 - ETA: 3s - loss: 0.6824 - acc: 0.662 - ETA: 3s - loss: 0.6814 - acc: 0.661 - ETA: 2s - loss: 0.6817 - acc: 0.661 - ETA: 2s - loss: 0.6812 - acc: 0.660 - ETA: 2s - loss: 0.6797 - acc: 0.661 - ETA: 2s - loss: 0.6769 - acc: 0.663 - ETA: 1s - loss: 0.6746 - acc: 0.665 - ETA: 1s - loss: 0.6740 - acc: 0.666 - ETA: 1s - loss: 0.6741 - acc: 0.665 - ETA: 1s - loss: 0.6736 - acc: 0.667 - ETA: 0s - loss: 0.6712 - acc: 0.669432/432 [==============================] - ETA: 0s - loss: 0.6682 - acc: 0.670 - ETA: 0s - loss: 0.6688 - acc: 0.669 - 66s 152ms/step - loss: 0.6664 - acc: 0.6713 - val_loss: 0.4094 - val_acc: 0.8333\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/432 [============================>.] - ETA: 1:09 - loss: 0.1218 - acc: 1.000 - ETA: 1:09 - loss: 0.1058 - acc: 1.000 - ETA: 1:11 - loss: 0.4810 - acc: 0.833 - ETA: 1:10 - loss: 0.5482 - acc: 0.750 - ETA: 1:10 - loss: 0.4778 - acc: 0.800 - ETA: 1:09 - loss: 0.4573 - acc: 0.833 - ETA: 1:08 - loss: 0.4009 - acc: 0.857 - ETA: 1:09 - loss: 0.3699 - acc: 0.875 - ETA: 1:08 - loss: 0.3407 - acc: 0.888 - ETA: 1:08 - loss: 0.3790 - acc: 0.850 - ETA: 1:07 - loss: 0.4403 - acc: 0.772 - ETA: 1:06 - loss: 0.4060 - acc: 0.791 - ETA: 1:06 - loss: 0.4206 - acc: 0.769 - ETA: 1:05 - loss: 0.3982 - acc: 0.785 - ETA: 1:04 - loss: 0.4171 - acc: 0.766 - ETA: 1:04 - loss: 0.4154 - acc: 0.781 - ETA: 1:03 - loss: 0.4115 - acc: 0.794 - ETA: 1:03 - loss: 0.4064 - acc: 0.805 - ETA: 1:03 - loss: 0.4158 - acc: 0.789 - ETA: 1:03 - loss: 0.4560 - acc: 0.750 - ETA: 1:03 - loss: 0.4518 - acc: 0.738 - ETA: 1:03 - loss: 0.4334 - acc: 0.750 - ETA: 1:03 - loss: 0.4257 - acc: 0.760 - ETA: 1:03 - loss: 0.4111 - acc: 0.770 - ETA: 1:03 - loss: 0.4166 - acc: 0.760 - ETA: 1:03 - loss: 0.4100 - acc: 0.769 - ETA: 1:03 - loss: 0.4285 - acc: 0.759 - ETA: 1:03 - loss: 0.4225 - acc: 0.767 - ETA: 1:02 - loss: 0.4090 - acc: 0.775 - ETA: 1:02 - loss: 0.4088 - acc: 0.783 - ETA: 1:01 - loss: 0.4148 - acc: 0.790 - ETA: 1:01 - loss: 0.4332 - acc: 0.781 - ETA: 1:00 - loss: 0.4229 - acc: 0.787 - ETA: 1:00 - loss: 0.4123 - acc: 0.794 - ETA: 1:00 - loss: 0.4039 - acc: 0.800 - ETA: 59s - loss: 0.3953 - acc: 0.805 - ETA: 59s - loss: 0.3991 - acc: 0.79 - ETA: 59s - loss: 0.3916 - acc: 0.80 - ETA: 59s - loss: 0.3876 - acc: 0.80 - ETA: 59s - loss: 0.4191 - acc: 0.80 - ETA: 59s - loss: 0.4307 - acc: 0.79 - ETA: 58s - loss: 0.4361 - acc: 0.78 - ETA: 58s - loss: 0.4344 - acc: 0.79 - ETA: 58s - loss: 0.4362 - acc: 0.78 - ETA: 57s - loss: 0.4381 - acc: 0.77 - ETA: 57s - loss: 0.4420 - acc: 0.77 - ETA: 56s - loss: 0.4403 - acc: 0.76 - ETA: 56s - loss: 0.4466 - acc: 0.76 - ETA: 55s - loss: 0.4411 - acc: 0.76 - ETA: 55s - loss: 0.4392 - acc: 0.77 - ETA: 55s - loss: 0.4313 - acc: 0.77 - ETA: 54s - loss: 0.4507 - acc: 0.76 - ETA: 54s - loss: 0.4497 - acc: 0.77 - ETA: 54s - loss: 0.4429 - acc: 0.77 - ETA: 53s - loss: 0.4369 - acc: 0.78 - ETA: 53s - loss: 0.4493 - acc: 0.76 - ETA: 52s - loss: 0.4537 - acc: 0.76 - ETA: 52s - loss: 0.4533 - acc: 0.75 - ETA: 52s - loss: 0.4495 - acc: 0.76 - ETA: 51s - loss: 0.4527 - acc: 0.75 - ETA: 51s - loss: 0.4470 - acc: 0.76 - ETA: 50s - loss: 0.4670 - acc: 0.75 - ETA: 50s - loss: 0.4647 - acc: 0.76 - ETA: 50s - loss: 0.4610 - acc: 0.76 - ETA: 49s - loss: 0.4600 - acc: 0.76 - ETA: 49s - loss: 0.4543 - acc: 0.76 - ETA: 49s - loss: 0.4538 - acc: 0.76 - ETA: 48s - loss: 0.4547 - acc: 0.76 - ETA: 48s - loss: 0.4495 - acc: 0.76 - ETA: 48s - loss: 0.4442 - acc: 0.77 - ETA: 48s - loss: 0.4423 - acc: 0.77 - ETA: 47s - loss: 0.4381 - acc: 0.77 - ETA: 47s - loss: 0.4395 - acc: 0.77 - ETA: 47s - loss: 0.4349 - acc: 0.77 - ETA: 47s - loss: 0.4328 - acc: 0.78 - ETA: 47s - loss: 0.4297 - acc: 0.78 - ETA: 47s - loss: 0.4255 - acc: 0.78 - ETA: 47s - loss: 0.4222 - acc: 0.78 - ETA: 46s - loss: 0.4175 - acc: 0.79 - ETA: 46s - loss: 0.4238 - acc: 0.78 - ETA: 46s - loss: 0.4230 - acc: 0.79 - ETA: 45s - loss: 0.4198 - acc: 0.79 - ETA: 45s - loss: 0.4158 - acc: 0.79 - ETA: 45s - loss: 0.4112 - acc: 0.79 - ETA: 44s - loss: 0.4284 - acc: 0.79 - ETA: 44s - loss: 0.4400 - acc: 0.79 - ETA: 44s - loss: 0.4431 - acc: 0.78 - ETA: 43s - loss: 0.4444 - acc: 0.78 - ETA: 43s - loss: 0.4406 - acc: 0.78 - ETA: 42s - loss: 0.4601 - acc: 0.77 - ETA: 42s - loss: 0.4581 - acc: 0.78 - ETA: 42s - loss: 0.4534 - acc: 0.78 - ETA: 42s - loss: 0.4628 - acc: 0.77 - ETA: 41s - loss: 0.4598 - acc: 0.78 - ETA: 41s - loss: 0.4550 - acc: 0.78 - ETA: 41s - loss: 0.4511 - acc: 0.78 - ETA: 40s - loss: 0.4595 - acc: 0.78 - ETA: 40s - loss: 0.4573 - acc: 0.78 - ETA: 39s - loss: 0.4538 - acc: 0.78 - ETA: 39s - loss: 0.4569 - acc: 0.78 - ETA: 39s - loss: 0.4553 - acc: 0.78 - ETA: 38s - loss: 0.4546 - acc: 0.78 - ETA: 38s - loss: 0.4516 - acc: 0.78 - ETA: 38s - loss: 0.4511 - acc: 0.78 - ETA: 38s - loss: 0.4500 - acc: 0.78 - ETA: 38s - loss: 0.4511 - acc: 0.78 - ETA: 38s - loss: 0.4472 - acc: 0.78 - ETA: 38s - loss: 0.4607 - acc: 0.77 - ETA: 37s - loss: 0.4649 - acc: 0.77 - ETA: 37s - loss: 0.4629 - acc: 0.77 - ETA: 37s - loss: 0.4640 - acc: 0.77 - ETA: 36s - loss: 0.4607 - acc: 0.77 - ETA: 36s - loss: 0.4619 - acc: 0.77 - ETA: 35s - loss: 0.4611 - acc: 0.77 - ETA: 35s - loss: 0.4573 - acc: 0.77 - ETA: 35s - loss: 0.4542 - acc: 0.77 - ETA: 34s - loss: 0.4506 - acc: 0.77 - ETA: 34s - loss: 0.4522 - acc: 0.77 - ETA: 33s - loss: 0.4514 - acc: 0.77 - ETA: 33s - loss: 0.4580 - acc: 0.77 - ETA: 33s - loss: 0.4615 - acc: 0.77 - ETA: 32s - loss: 0.4596 - acc: 0.77 - ETA: 32s - loss: 0.4637 - acc: 0.76 - ETA: 32s - loss: 0.4624 - acc: 0.77 - ETA: 31s - loss: 0.4607 - acc: 0.77 - ETA: 31s - loss: 0.4596 - acc: 0.77 - ETA: 30s - loss: 0.4682 - acc: 0.76 - ETA: 30s - loss: 0.4676 - acc: 0.76 - ETA: 30s - loss: 0.4657 - acc: 0.77 - ETA: 29s - loss: 0.4655 - acc: 0.76 - ETA: 29s - loss: 0.4666 - acc: 0.76 - ETA: 29s - loss: 0.4632 - acc: 0.76 - ETA: 28s - loss: 0.4603 - acc: 0.77 - ETA: 28s - loss: 0.4584 - acc: 0.77 - ETA: 27s - loss: 0.4553 - acc: 0.77 - ETA: 27s - loss: 0.4525 - acc: 0.77 - ETA: 27s - loss: 0.4495 - acc: 0.77 - ETA: 26s - loss: 0.4496 - acc: 0.77 - ETA: 26s - loss: 0.4479 - acc: 0.78 - ETA: 26s - loss: 0.4451 - acc: 0.78 - ETA: 25s - loss: 0.4452 - acc: 0.78 - ETA: 25s - loss: 0.4422 - acc: 0.78 - ETA: 24s - loss: 0.4443 - acc: 0.77 - ETA: 24s - loss: 0.4457 - acc: 0.77 - ETA: 24s - loss: 0.4504 - acc: 0.77 - ETA: 23s - loss: 0.4480 - acc: 0.77 - ETA: 23s - loss: 0.4464 - acc: 0.77 - ETA: 23s - loss: 0.4462 - acc: 0.77 - ETA: 22s - loss: 0.4448 - acc: 0.77 - ETA: 22s - loss: 0.4423 - acc: 0.77 - ETA: 22s - loss: 0.4484 - acc: 0.77 - ETA: 21s - loss: 0.4456 - acc: 0.77 - ETA: 21s - loss: 0.4441 - acc: 0.77 - ETA: 21s - loss: 0.4416 - acc: 0.77 - ETA: 20s - loss: 0.4423 - acc: 0.77 - ETA: 20s - loss: 0.4400 - acc: 0.77 - ETA: 19s - loss: 0.4376 - acc: 0.77 - ETA: 19s - loss: 0.4353 - acc: 0.77 - ETA: 19s - loss: 0.4327 - acc: 0.77 - ETA: 18s - loss: 0.4302 - acc: 0.78 - ETA: 18s - loss: 0.4359 - acc: 0.77 - ETA: 18s - loss: 0.4399 - acc: 0.77 - ETA: 17s - loss: 0.4374 - acc: 0.77 - ETA: 17s - loss: 0.4353 - acc: 0.78 - ETA: 17s - loss: 0.4379 - acc: 0.77 - ETA: 16s - loss: 0.4353 - acc: 0.78 - ETA: 16s - loss: 0.4341 - acc: 0.78 - ETA: 16s - loss: 0.4334 - acc: 0.78 - ETA: 15s - loss: 0.4355 - acc: 0.78 - ETA: 15s - loss: 0.4373 - acc: 0.77 - ETA: 15s - loss: 0.4351 - acc: 0.78 - ETA: 14s - loss: 0.4331 - acc: 0.78 - ETA: 14s - loss: 0.4333 - acc: 0.78 - ETA: 14s - loss: 0.4357 - acc: 0.77 - ETA: 13s - loss: 0.4487 - acc: 0.77 - ETA: 13s - loss: 0.4466 - acc: 0.77 - ETA: 13s - loss: 0.4449 - acc: 0.77 - ETA: 12s - loss: 0.4470 - acc: 0.77 - ETA: 12s - loss: 0.4467 - acc: 0.77 - ETA: 12s - loss: 0.4467 - acc: 0.77 - ETA: 11s - loss: 0.4453 - acc: 0.77 - ETA: 11s - loss: 0.4438 - acc: 0.77 - ETA: 11s - loss: 0.4450 - acc: 0.77 - ETA: 10s - loss: 0.4436 - acc: 0.77 - ETA: 10s - loss: 0.4422 - acc: 0.77 - ETA: 10s - loss: 0.4401 - acc: 0.77 - ETA: 9s - loss: 0.4385 - acc: 0.7807 - ETA: 9s - loss: 0.4385 - acc: 0.779 - ETA: 9s - loss: 0.4404 - acc: 0.777 - ETA: 8s - loss: 0.4414 - acc: 0.776 - ETA: 8s - loss: 0.4394 - acc: 0.777 - ETA: 8s - loss: 0.4389 - acc: 0.778 - ETA: 7s - loss: 0.4368 - acc: 0.779 - ETA: 7s - loss: 0.4360 - acc: 0.780 - ETA: 7s - loss: 0.4370 - acc: 0.779 - ETA: 6s - loss: 0.4352 - acc: 0.780 - ETA: 6s - loss: 0.4361 - acc: 0.779 - ETA: 6s - loss: 0.4346 - acc: 0.780 - ETA: 5s - loss: 0.4332 - acc: 0.781 - ETA: 5s - loss: 0.4314 - acc: 0.782 - ETA: 5s - loss: 0.4303 - acc: 0.783 - ETA: 4s - loss: 0.4333 - acc: 0.782 - ETA: 4s - loss: 0.4402 - acc: 0.778 - ETA: 4s - loss: 0.4394 - acc: 0.779 - ETA: 3s - loss: 0.4375 - acc: 0.780 - ETA: 3s - loss: 0.4359 - acc: 0.781 - ETA: 3s - loss: 0.4341 - acc: 0.782 - ETA: 2s - loss: 0.4321 - acc: 0.783 - ETA: 2s - loss: 0.4314 - acc: 0.784 - ETA: 2s - loss: 0.4295 - acc: 0.785 - ETA: 1s - loss: 0.4276 - acc: 0.786 - ETA: 1s - loss: 0.4285 - acc: 0.785 - ETA: 1s - loss: 0.4268 - acc: 0.7864"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - ETA: 0s - loss: 0.4250 - acc: 0.787 - ETA: 0s - loss: 0.4235 - acc: 0.788 - 79s 183ms/step - loss: 0.4266 - acc: 0.7870 - val_loss: 0.8112 - val_acc: 0.6042\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428/432 [============================>.] - ETA: 1:13 - loss: 0.8041 - acc: 0.500 - ETA: 1:08 - loss: 0.4655 - acc: 0.750 - ETA: 1:07 - loss: 0.5975 - acc: 0.666 - ETA: 1:08 - loss: 0.4815 - acc: 0.750 - ETA: 1:07 - loss: 0.4148 - acc: 0.800 - ETA: 1:06 - loss: 0.3940 - acc: 0.833 - ETA: 1:05 - loss: 0.4377 - acc: 0.785 - ETA: 1:06 - loss: 0.3923 - acc: 0.812 - ETA: 1:05 - loss: 0.4390 - acc: 0.777 - ETA: 1:05 - loss: 0.4110 - acc: 0.800 - ETA: 1:04 - loss: 0.3778 - acc: 0.818 - ETA: 1:04 - loss: 0.3618 - acc: 0.833 - ETA: 1:03 - loss: 0.3488 - acc: 0.846 - ETA: 1:02 - loss: 0.3581 - acc: 0.821 - ETA: 1:02 - loss: 0.3424 - acc: 0.833 - ETA: 1:02 - loss: 0.3224 - acc: 0.843 - ETA: 1:01 - loss: 0.3044 - acc: 0.852 - ETA: 1:01 - loss: 0.3001 - acc: 0.861 - ETA: 1:01 - loss: 0.2897 - acc: 0.868 - ETA: 1:00 - loss: 0.2913 - acc: 0.875 - ETA: 1:00 - loss: 0.2776 - acc: 0.881 - ETA: 1:00 - loss: 0.3216 - acc: 0.840 - ETA: 1:00 - loss: 0.3089 - acc: 0.847 - ETA: 1:00 - loss: 0.3148 - acc: 0.854 - ETA: 1:00 - loss: 0.3061 - acc: 0.860 - ETA: 1:00 - loss: 0.3233 - acc: 0.846 - ETA: 1:00 - loss: 0.3148 - acc: 0.851 - ETA: 59s - loss: 0.3137 - acc: 0.857 - ETA: 59s - loss: 0.3551 - acc: 0.84 - ETA: 59s - loss: 0.3578 - acc: 0.83 - ETA: 58s - loss: 0.3703 - acc: 0.82 - ETA: 58s - loss: 0.3628 - acc: 0.82 - ETA: 58s - loss: 0.3548 - acc: 0.83 - ETA: 57s - loss: 0.3462 - acc: 0.83 - ETA: 57s - loss: 0.3578 - acc: 0.82 - ETA: 57s - loss: 0.3503 - acc: 0.83 - ETA: 56s - loss: 0.3433 - acc: 0.83 - ETA: 56s - loss: 0.3636 - acc: 0.82 - ETA: 56s - loss: 0.3723 - acc: 0.82 - ETA: 56s - loss: 0.3648 - acc: 0.82 - ETA: 55s - loss: 0.3594 - acc: 0.82 - ETA: 55s - loss: 0.3551 - acc: 0.83 - ETA: 55s - loss: 0.3469 - acc: 0.83 - ETA: 54s - loss: 0.3391 - acc: 0.84 - ETA: 54s - loss: 0.3813 - acc: 0.83 - ETA: 54s - loss: 0.3770 - acc: 0.83 - ETA: 54s - loss: 0.3761 - acc: 0.84 - ETA: 53s - loss: 0.3917 - acc: 0.83 - ETA: 53s - loss: 0.4050 - acc: 0.82 - ETA: 53s - loss: 0.3985 - acc: 0.83 - ETA: 53s - loss: 0.4143 - acc: 0.82 - ETA: 52s - loss: 0.4135 - acc: 0.81 - ETA: 52s - loss: 0.4117 - acc: 0.82 - ETA: 52s - loss: 0.4071 - acc: 0.82 - ETA: 52s - loss: 0.4009 - acc: 0.82 - ETA: 52s - loss: 0.4016 - acc: 0.83 - ETA: 51s - loss: 0.3962 - acc: 0.83 - ETA: 51s - loss: 0.4143 - acc: 0.81 - ETA: 51s - loss: 0.4131 - acc: 0.82 - ETA: 51s - loss: 0.4159 - acc: 0.81 - ETA: 50s - loss: 0.4140 - acc: 0.81 - ETA: 50s - loss: 0.4174 - acc: 0.81 - ETA: 50s - loss: 0.4278 - acc: 0.80 - ETA: 49s - loss: 0.4258 - acc: 0.81 - ETA: 49s - loss: 0.4198 - acc: 0.81 - ETA: 49s - loss: 0.4154 - acc: 0.81 - ETA: 48s - loss: 0.4121 - acc: 0.82 - ETA: 48s - loss: 0.4072 - acc: 0.82 - ETA: 48s - loss: 0.4038 - acc: 0.82 - ETA: 47s - loss: 0.4015 - acc: 0.82 - ETA: 47s - loss: 0.4007 - acc: 0.83 - ETA: 47s - loss: 0.3984 - acc: 0.83 - ETA: 46s - loss: 0.4001 - acc: 0.82 - ETA: 46s - loss: 0.4083 - acc: 0.82 - ETA: 46s - loss: 0.4033 - acc: 0.82 - ETA: 45s - loss: 0.3995 - acc: 0.82 - ETA: 45s - loss: 0.4011 - acc: 0.82 - ETA: 45s - loss: 0.4002 - acc: 0.82 - ETA: 44s - loss: 0.3967 - acc: 0.82 - ETA: 44s - loss: 0.3924 - acc: 0.83 - ETA: 44s - loss: 0.3879 - acc: 0.83 - ETA: 43s - loss: 0.3833 - acc: 0.83 - ETA: 43s - loss: 0.3789 - acc: 0.83 - ETA: 43s - loss: 0.3764 - acc: 0.83 - ETA: 42s - loss: 0.3723 - acc: 0.84 - ETA: 42s - loss: 0.3682 - acc: 0.84 - ETA: 42s - loss: 0.3641 - acc: 0.84 - ETA: 41s - loss: 0.3620 - acc: 0.84 - ETA: 41s - loss: 0.3647 - acc: 0.84 - ETA: 41s - loss: 0.3753 - acc: 0.83 - ETA: 40s - loss: 0.3740 - acc: 0.84 - ETA: 40s - loss: 0.3730 - acc: 0.84 - ETA: 40s - loss: 0.3777 - acc: 0.83 - ETA: 39s - loss: 0.3738 - acc: 0.84 - ETA: 39s - loss: 0.3707 - acc: 0.84 - ETA: 39s - loss: 0.3681 - acc: 0.84 - ETA: 38s - loss: 0.3661 - acc: 0.84 - ETA: 38s - loss: 0.3681 - acc: 0.84 - ETA: 38s - loss: 0.3667 - acc: 0.84 - ETA: 37s - loss: 0.3660 - acc: 0.84 - ETA: 37s - loss: 0.3689 - acc: 0.84 - ETA: 37s - loss: 0.3753 - acc: 0.83 - ETA: 36s - loss: 0.3727 - acc: 0.83 - ETA: 36s - loss: 0.3807 - acc: 0.83 - ETA: 36s - loss: 0.3996 - acc: 0.83 - ETA: 35s - loss: 0.3975 - acc: 0.83 - ETA: 35s - loss: 0.3939 - acc: 0.83 - ETA: 35s - loss: 0.3964 - acc: 0.83 - ETA: 34s - loss: 0.3964 - acc: 0.83 - ETA: 34s - loss: 0.3929 - acc: 0.83 - ETA: 34s - loss: 0.3898 - acc: 0.83 - ETA: 33s - loss: 0.3873 - acc: 0.83 - ETA: 33s - loss: 0.3846 - acc: 0.83 - ETA: 33s - loss: 0.3815 - acc: 0.83 - ETA: 32s - loss: 0.3805 - acc: 0.83 - ETA: 32s - loss: 0.3783 - acc: 0.84 - ETA: 32s - loss: 0.3794 - acc: 0.83 - ETA: 31s - loss: 0.3769 - acc: 0.83 - ETA: 31s - loss: 0.3741 - acc: 0.84 - ETA: 31s - loss: 0.3770 - acc: 0.83 - ETA: 30s - loss: 0.3741 - acc: 0.83 - ETA: 30s - loss: 0.3713 - acc: 0.84 - ETA: 30s - loss: 0.3723 - acc: 0.83 - ETA: 29s - loss: 0.3700 - acc: 0.83 - ETA: 29s - loss: 0.3672 - acc: 0.84 - ETA: 29s - loss: 0.3680 - acc: 0.83 - ETA: 28s - loss: 0.3691 - acc: 0.83 - ETA: 28s - loss: 0.3684 - acc: 0.83 - ETA: 28s - loss: 0.3695 - acc: 0.83 - ETA: 27s - loss: 0.3671 - acc: 0.83 - ETA: 27s - loss: 0.3647 - acc: 0.83 - ETA: 27s - loss: 0.3641 - acc: 0.84 - ETA: 26s - loss: 0.3629 - acc: 0.84 - ETA: 26s - loss: 0.3625 - acc: 0.84 - ETA: 26s - loss: 0.3640 - acc: 0.84 - ETA: 25s - loss: 0.3665 - acc: 0.83 - ETA: 25s - loss: 0.3642 - acc: 0.83 - ETA: 25s - loss: 0.3627 - acc: 0.84 - ETA: 24s - loss: 0.3614 - acc: 0.84 - ETA: 24s - loss: 0.3596 - acc: 0.84 - ETA: 24s - loss: 0.3571 - acc: 0.84 - ETA: 23s - loss: 0.3557 - acc: 0.84 - ETA: 23s - loss: 0.3618 - acc: 0.84 - ETA: 23s - loss: 0.3601 - acc: 0.84 - ETA: 22s - loss: 0.3629 - acc: 0.84 - ETA: 22s - loss: 0.3615 - acc: 0.84 - ETA: 22s - loss: 0.3591 - acc: 0.84 - ETA: 21s - loss: 0.3571 - acc: 0.84 - ETA: 21s - loss: 0.3548 - acc: 0.84 - ETA: 21s - loss: 0.3560 - acc: 0.84 - ETA: 20s - loss: 0.3546 - acc: 0.84 - ETA: 20s - loss: 0.3529 - acc: 0.84 - ETA: 20s - loss: 0.3513 - acc: 0.84 - ETA: 19s - loss: 0.3500 - acc: 0.84 - ETA: 19s - loss: 0.3477 - acc: 0.84 - ETA: 19s - loss: 0.3466 - acc: 0.84 - ETA: 18s - loss: 0.3448 - acc: 0.85 - ETA: 18s - loss: 0.3433 - acc: 0.85 - ETA: 18s - loss: 0.3429 - acc: 0.85 - ETA: 17s - loss: 0.3420 - acc: 0.85 - ETA: 17s - loss: 0.3400 - acc: 0.85 - ETA: 17s - loss: 0.3477 - acc: 0.85 - ETA: 17s - loss: 0.3462 - acc: 0.85 - ETA: 16s - loss: 0.3441 - acc: 0.85 - ETA: 16s - loss: 0.3423 - acc: 0.85 - ETA: 16s - loss: 0.3407 - acc: 0.85 - ETA: 15s - loss: 0.3388 - acc: 0.85 - ETA: 15s - loss: 0.3394 - acc: 0.85 - ETA: 15s - loss: 0.3375 - acc: 0.85 - ETA: 14s - loss: 0.3355 - acc: 0.85 - ETA: 14s - loss: 0.3337 - acc: 0.85 - ETA: 14s - loss: 0.3508 - acc: 0.85 - ETA: 13s - loss: 0.3489 - acc: 0.85 - ETA: 13s - loss: 0.3470 - acc: 0.85 - ETA: 13s - loss: 0.3467 - acc: 0.85 - ETA: 12s - loss: 0.3447 - acc: 0.85 - ETA: 12s - loss: 0.3483 - acc: 0.85 - ETA: 12s - loss: 0.3499 - acc: 0.85 - ETA: 11s - loss: 0.3492 - acc: 0.85 - ETA: 11s - loss: 0.3500 - acc: 0.85 - ETA: 11s - loss: 0.3482 - acc: 0.85 - ETA: 10s - loss: 0.3466 - acc: 0.85 - ETA: 10s - loss: 0.3471 - acc: 0.84 - ETA: 10s - loss: 0.3539 - acc: 0.84 - ETA: 9s - loss: 0.3520 - acc: 0.8486 - ETA: 9s - loss: 0.3513 - acc: 0.849 - ETA: 9s - loss: 0.3497 - acc: 0.850 - ETA: 9s - loss: 0.3489 - acc: 0.851 - ETA: 8s - loss: 0.3472 - acc: 0.851 - ETA: 8s - loss: 0.3456 - acc: 0.852 - ETA: 8s - loss: 0.3458 - acc: 0.850 - ETA: 7s - loss: 0.3445 - acc: 0.851 - ETA: 7s - loss: 0.3429 - acc: 0.852 - ETA: 7s - loss: 0.3418 - acc: 0.853 - ETA: 6s - loss: 0.3483 - acc: 0.851 - ETA: 6s - loss: 0.3467 - acc: 0.852 - ETA: 6s - loss: 0.3500 - acc: 0.850 - ETA: 5s - loss: 0.3485 - acc: 0.851 - ETA: 5s - loss: 0.3468 - acc: 0.851 - ETA: 5s - loss: 0.3465 - acc: 0.852 - ETA: 4s - loss: 0.3451 - acc: 0.853 - ETA: 4s - loss: 0.3435 - acc: 0.854 - ETA: 4s - loss: 0.3566 - acc: 0.849 - ETA: 3s - loss: 0.3593 - acc: 0.848 - ETA: 3s - loss: 0.3579 - acc: 0.848 - ETA: 3s - loss: 0.3566 - acc: 0.849 - ETA: 2s - loss: 0.3567 - acc: 0.847 - ETA: 2s - loss: 0.3559 - acc: 0.848 - ETA: 2s - loss: 0.3544 - acc: 0.849 - ETA: 1s - loss: 0.3528 - acc: 0.850 - ETA: 1s - loss: 0.3513 - acc: 0.850 - ETA: 1s - loss: 0.3538 - acc: 0.849 - ETA: 0s - loss: 0.3523 - acc: 0.849 - ETA: 0s - loss: 0.3517 - acc: 0.8505432/432 [==============================] - ETA: 0s - loss: 0.3509 - acc: 0.851 - 78s 179ms/step - loss: 0.3514 - acc: 0.8519 - val_loss: 0.5026 - val_acc: 0.7708\n",
      "120/120 [==============================] - ETA: 28 - ETA: 23 - ETA: 21 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 18s 152ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-08 13:44:37,384] A new study created in memory with name: CatsAndDogs_Simple\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-08 13:44:37.389576] [OptKeras] Ready for optimization. (message printed as verbose is set to 1+)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:69: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-08 13:44:38.171433]  (None) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-08 13:48:47,132] Trial 0 finished with value: 0.6666666666666666 and parameters: {'activation': 'linear', 'optimizer': 'SGD'}. Best is trial 0 with value: 0.6666666666666666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-08 13:48:47.217852] Trial#: 0, value: 6.666667e-01| Best trial#: 0, value: 6.666667e-01, params: {'activation': 'linear', 'optimizer': 'SGD'}\n"
     ]
    }
   ],
   "source": [
    "from optkeras.optkeras import OptKeras\n",
    "import optkeras\n",
    "import pickle\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input,Dense,BatchNormalization,Flatten,Dropout,GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import layer_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model,load_model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import optuna\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "nb_classes = 2\n",
    "epochs=3\n",
    "batch_size =2\n",
    "\n",
    "vgg16_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "x = vgg16_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(nb_classes, activation = 'softmax')(x)\n",
    "model = Model(input = vgg16_model.input, output = predictions)\n",
    "\n",
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer = 'rmsprop',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model_info = model.fit(x=train_photos, y=train_labels,batch_size=2 , epochs=epochs, \n",
    "                   verbose=1,validation_data=(val_photos,val_labels))\n",
    "\n",
    "model.save('model_hpo.h5')\n",
    "\n",
    "m= load_model(\"model_hpo.h5\")\n",
    "test_photos = test_photos.astype(\"float32\")\n",
    "\n",
    "results = m.evaluate(test_photos, test_labels, batch_size=1)\n",
    "optkeras.optkeras.get_trial_default = lambda: optuna.trial.FrozenTrial(\n",
    "            None, None, None, None, None, None, None, None, None, None, None)\n",
    "study_name = \"CatsAndDogs\" + '_Simple'\n",
    "ok = OptKeras(study_name=study_name,\n",
    "              monitor='val_acc',\n",
    "              direction='maximize')\n",
    "\n",
    "def objective(trial):\n",
    "    nb_classes = 2\n",
    "    epochs=3\n",
    "    batch_size =2\n",
    "\n",
    "    vgg16_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "    x = vgg16_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation=trial.suggest_categorical('activation', ['relu', 'linear']))(x)\n",
    "    predictions = Dense(nb_classes, activation = 'softmax')(x)\n",
    "    model = Model(input = vgg16_model.input, output = predictions)\n",
    "    for layer in vgg16_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer = trial.suggest_categorical(\"optimizer\", [\"rmsprop\", \"Adam\", \"SGD\"]),loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    model.fit(x=train_photos, y=train_labels,batch_size=2 , epochs=epochs, callbacks=ok.callbacks(trial),\n",
    "              verbose=ok.keras_verbose ,validation_data=(val_photos,val_labels))\n",
    "    return ok.trial_best_value\n",
    "\n",
    "ok.optimize(objective, n_trials=3, timeout = 3*60)\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_rows = 8 \n",
    "\n",
    "pd.options.display.max_rows = 8\n",
    "\n",
    "put_csv = pd.read_csv(ok.keras_log_file_path)\n",
    "put_csv.to_csv(optuna_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:69: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-08 13:48:48.189721] Trial#: 0, value: 6.666667e-01| Best trial#: 0, value: 6.666667e-01, params: {'activation': 'linear', 'optimizer': 'SGD'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-08 13:53:39,267] Trial 1 finished with value: 0.8958333333333334 and parameters: {'activation': 'linear', 'optimizer': 'Adam'}. Best is trial 1 with value: 0.8958333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-08 13:53:39.355121] Trial#: 1, value: 8.958333e-01| Best trial#: 1, value: 8.958333e-01, params: {'activation': 'linear', 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "ok.optimize(objective, n_trials=4, timeout = 3*60)\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_rows = 8 \n",
    "\n",
    "put_csv = pd.read_csv(ok.keras_log_file_path)\n",
    "put_csv.to_csv(optuna_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import joblib\n",
    "MODEL = \"basicnet\"\n",
    "EPOCHS = 5\n",
    "CHECKPOINTS_FILES_LIST = []\n",
    "STUDY = None\n",
    "N_TRIALS = 5\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Binary Classification of Images')\n",
    "parser.add_argument('epochs',  metavar='num_epochs', type=int, nargs=1, help = \"number of training epochs\")\n",
    "parser.add_argument('trials',  metavar='num_trials', type=int, nargs=1, help = \"number of HPO trials\")\n",
    "\n",
    "def hpo_monitor(study, trial):\n",
    "    joblib.dump(study,\"hpo_study_checkpoint_\" + MODEL + \".pkl\")\n",
    "    \n",
    "def hpo_monitor_sigterm(final_prefix = \"\"):\n",
    "    joblib.dump(STUDY,final_prefix + \"hpo_study_checkpoint_\" + MODEL + \".pkl\")\n",
    "    \n",
    "def objective(trial):\n",
    "    nb_classes = 2\n",
    "    epochs=3\n",
    "    batch_size =2\n",
    "    optimizer_options = [\"RMSprop\", \"Adam\", \"SGD\"]\n",
    "    losses_dict= {'train': {}, 'test': {}, 'accuracy': {}}\n",
    "\n",
    "    vgg16_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "    x = vgg16_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation=trial.suggest_categorical('activation', ['relu', 'linear']))(x)\n",
    "    predictions = Dense(nb_classes, activation = 'softmax')(x)\n",
    "    model = Model(input = vgg16_model.input, output = predictions)\n",
    "    for layer in vgg16_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer = trial.suggest_categorical(\"optimizer\", [\"rmsprop\", \"Adam\", \"SGD\"]),loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    model.fit(x=train_photos, y=train_labels,batch_size=2 , epochs=epochs, callbacks=ok.callbacks(trial),\n",
    "              verbose=ok.keras_verbose ,validation_data=(val_photos,val_labels))\n",
    "    return ok.trial_best_value\n",
    "\n",
    "def check_status_last_trial():\n",
    "    global STUDY\n",
    "    last_trial_index = len(STUDY.trials) -1\n",
    "    last_trial_status = str(STUDY.trials[last_trial_index].state)\n",
    "    \n",
    "def create_study(hpo_checkpoint_file):\n",
    "    global STUDY\n",
    "    load_checkpoint_flag = True\n",
    "    \n",
    "    if len(CHECKPOINTS_FILES_LIST) == 0:\n",
    "        load_checkpoint_flag = False\n",
    "\n",
    "    if load_checkpoint_flag:\n",
    "        #print(\"Loading an existing study...\")\n",
    "        STUDY = joblib.load(\"hpo_study_checkpoint_\" + MODEL + \".pkl\")\n",
    "        #check_status_last_trial()\n",
    "        todo_trials = N_TRIALS - len(STUDY.trials_dataframe())\n",
    "        if todo_trials > 0 :\n",
    "            #print(\"There are {} trials to do out of {}\".format(todo_trials, N_TRIALS))\n",
    "            STUDY.optimize(objective, n_trials=todo_trials, timeout=600, callbacks=[hpo_monitor])\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"This study is finished. Nothing to do.\")\n",
    "    else:\n",
    "        STUDY = optuna.create_study(direction = 'maximize', study_name = MODEL)\n",
    "        STUDY.optimize(objective, n_trials=N_TRIALS, timeout=600, callbacks=[hpo_monitor])\n",
    "\n",
    "def main():\n",
    "\n",
    "    global MODEL\n",
    "    global EPOCHS\n",
    "    global CHECKPOINTS_FILES_LIST\n",
    "    global N_TRIALS\n",
    "\n",
    "    try:\n",
    "        signal.signal(signal.SIGTERM, sigterm_handler)\n",
    "        \n",
    "        args     = parser.parse_args()\n",
    "        EPOCHS   = args.epochs[0]\n",
    "        N_TRIALS = args.trials[0]\n",
    "\n",
    "        hpo_checkpoint_file = \"hpo_study_checkpoint_\" + MODEL + \".pkl\"\n",
    "        create_study(hpo_checkpoint_file)\n",
    "\n",
    "    except Exception as e:\n",
    "    \tpass\n",
    "    finally:\n",
    "        hpo_monitor_sigterm(\"final_\")\n",
    "\n",
    "  \n",
    "    return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
