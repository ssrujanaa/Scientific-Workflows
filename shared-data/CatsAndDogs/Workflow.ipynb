{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import requests from optkeras.optkeras import OptKeras\n",
    "import optkeras\n",
    "import pickle\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input,Dense,BatchNormalization,Flatten,Dropout,GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import layer_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model,load_model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import optuna\n",
    "from zipfile import ZipFile\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "#Import Pegasus API\n",
    "from Pegasus.api import *\n",
    "\n",
    "#Downloading dataset\n",
    "zipurl = 'https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip'    \n",
    "r = requests.get(zipurl, stream = True) \n",
    "  \n",
    "with open(\"dataset.zip\",\"wb\") as pdf: \n",
    "    for chunk in r.iter_content(chunk_size=1024): \n",
    "         if chunk: \n",
    "            pdf.write(chunk)\n",
    "            \n",
    "# Extract all the contents of zip file in current directory            \n",
    "with ZipFile('dataset.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optkeras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-119cfe485945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moptkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptKeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optkeras'"
     ]
    }
   ],
   "source": [
    "from optkeras.optkeras import OptKeras\n",
    "import optkeras\n",
    "import pickle\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input,Dense,BatchNormalization,Flatten,Dropout,GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import layer_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model,load_model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /usr/local/lib/python3.6/site-packages (20.2.2)\r\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.1.5\n",
      "  Downloading Keras-2.1.5-py2.py3-none-any.whl (334 kB)\n",
      "\u001b[K     |████████████████████████████████| 334 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow\n",
      "  Downloading tensorflow-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 320.4 MB 34 kB/s s eta 0:00:01     |█████████████████████████████▍  | 294.4 MB 34.3 MB/s eta 0:00:01     |███████████████████████████████▊| 317.2 MB 22.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.19.1-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 37.1 MB/s eta 0:00:01    |███████                         | 3.2 MB 37.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 33.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow\n",
      "  Downloading Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 40.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opencv-python\n",
      "  Downloading opencv_python-4.4.0.42-cp36-cp36m-manylinux2014_x86_64.whl (49.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 49.4 MB 31.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scipy>=0.14\n",
      "  Downloading scipy-1.5.2-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 38.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/lib/python3.6/site-packages (from keras==2.1.5) (1.14.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib64/python3.6/site-packages (from keras==2.1.5) (3.13)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 8.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 30.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting wheel>=0.26\n",
      "  Downloading wheel-0.35.1-py2.py3-none-any.whl (33 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.31.0-cp36-cp36m-manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 31.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 24.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 49.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.13.0-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 44.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 23.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas) (2020.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3.6/site-packages (from protobuf>=3.9.2->tensorflow) (39.2.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.20.1-py2.py3-none-any.whl (91 kB)\n",
      "\u001b[K     |████████████████████████████████| 91 kB 14.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 39.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "\u001b[K     |████████████████████████████████| 300 kB 26.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.10)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 6.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 32.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 9.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 42.9 MB/s eta 0:00:01\n",
      "\u001b[?25hUsing legacy 'setup.py install' for sklearn, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for wrapt, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for termcolor, since package 'wheel' is not installed.\n",
      "Installing collected packages: numpy, scipy, keras, opt-einsum, google-pasta, h5py, keras-preprocessing, wrapt, gast, wheel, grpcio, tensorflow-estimator, astunparse, absl-py, protobuf, markdown, tensorboard-plugin-wit, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, tensorboard, termcolor, tensorflow, pandas, pillow, opencv-python, joblib, threadpoolctl, scikit-learn, sklearn\n",
      "    Running setup.py install for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for sklearn ... \u001b[?25ldone\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "google-auth 1.20.1 requires setuptools>=40.3.0, but you'll have setuptools 39.2.0 which is incompatible.\n",
      "tensorboard 2.3.0 requires setuptools>=41.0.0, but you'll have setuptools 39.2.0 which is incompatible.\n",
      "tensorflow 2.3.0 requires numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.1 which is incompatible.\n",
      "tensorflow 2.3.0 requires scipy==1.4.1, but you'll have scipy 1.5.2 which is incompatible.\u001b[0m\n",
      "\u001b[?25hSuccessfully installed absl-py-0.10.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.20.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.31.0 h5py-2.10.0 joblib-0.16.0 keras-2.1.5 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.19.1 oauthlib-3.1.0 opencv-python-4.4.0.42 opt-einsum-3.3.0 pandas-1.1.1 pillow-7.2.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 scikit-learn-0.23.2 scipy-1.5.2 sklearn-0.0 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0 termcolor-1.1.0 threadpoolctl-2.1.0 werkzeug-1.0.1 wheel-0.35.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install  keras==2.1.5 tensorflow numpy pandas pillow opencv-python sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.1.5 in /usr/local/lib/python3.6/site-packages (2.1.5)\n",
      "Collecting tensorflow==1.13.1\n",
      "  Downloading tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 92.5 MB 18.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib64/python3.6/site-packages (1.19.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib64/python3.6/site-packages (1.1.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib64/python3.6/site-packages (7.2.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/site-packages (0.0)\n",
      "Collecting optuna\n",
      "  Downloading optuna-2.0.0.tar.gz (226 kB)\n",
      "\u001b[K     |████████████████████████████████| 226 kB 46.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-image\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 27.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ray\n",
      "  Downloading ray-0.8.7-cp36-cp36m-manylinux1_x86_64.whl (22.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.0 MB 44.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/lib64/python3.6/site-packages (from keras==2.1.5) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3.6/site-packages (from keras==2.1.5) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib64/python3.6/site-packages (from keras==2.1.5) (1.5.2)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/site-packages (from tensorflow==1.13.1) (0.3.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorflow==1.13.1) (0.35.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/site-packages (from tensorflow==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/site-packages (from tensorflow==1.13.1) (1.1.2)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/site-packages (from tensorflow==1.13.1) (0.10.0)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
      "\u001b[K     |████████████████████████████████| 367 kB 23.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n",
      "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib64/python3.6/site-packages (from tensorflow==1.13.1) (3.13.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib64/python3.6/site-packages (from tensorflow==1.13.1) (1.31.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib64/python3.6/site-packages (from sklearn) (0.23.2)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.4.2.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 21.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cliff\n",
      "  Downloading cliff-3.4.0-py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 8.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cmaes>=0.5.1\n",
      "  Downloading cmaes-0.6.0-py3-none-any.whl (9.6 kB)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-4.2.1-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/site-packages (from optuna) (0.16.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/site-packages (from optuna) (20.4)\n",
      "Collecting sqlalchemy>=1.1.0\n",
      "  Downloading SQLAlchemy-1.3.19-cp36-cp36m-manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 25.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.48.2-py2.py3-none-any.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib!=3.0.0,>=2.0.0\n",
      "  Downloading matplotlib-3.3.1-cp36-cp36m-manylinux1_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 31.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imageio>=2.3.0\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 40.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.8.25-py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.0\n",
      "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 36.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 27.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.6/site-packages (from ray) (3.2.0)\n",
      "Collecting redis<3.5.0,>=3.3.2\n",
      "  Downloading redis-3.4.1-py2.py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opencensus\n",
      "  Downloading opencensus-0.7.10-py2.py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 51.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting msgpack<2.0.0,>=1.0.0\n",
      "  Downloading msgpack-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (274 kB)\n",
      "\u001b[K     |████████████████████████████████| 274 kB 46.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorful\n",
      "  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n",
      "\u001b[K     |████████████████████████████████| 201 kB 34.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.6/site-packages (from ray) (0.8.0)\n",
      "Collecting click>=7.0\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 2.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from ray) (2.24.0)\n",
      "Collecting aioredis\n",
      "  Downloading aioredis-1.3.1-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 6.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting colorama\n",
      "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
      "Collecting py-spy>=0.2.0\n",
      "  Downloading py_spy-0.3.3-py2.py3-none-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 32.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/site-packages (from ray) (3.0.12)\n",
      "Collecting gpustat\n",
      "  Downloading gpustat-0.6.0.tar.gz (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 4.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting google\n",
      "  Downloading google-3.0.0-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 4.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.6.2-cp36-cp36m-manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 34.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib64/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
      "Collecting mock>=2.0.0\n",
      "  Downloading mock-4.0.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (39.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.3-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 7.6 MB/s  eta 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.4.5-py2.py3-none-any.whl (110 kB)\n",
      "\u001b[K     |████████████████████████████████| 110 kB 30.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cmd2!=0.8.3,>=0.8.0\n",
      "  Downloading cmd2-1.3.5-py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 33.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/site-packages (from cliff->optuna) (2.4.7)\n",
      "Collecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.2.0-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting PrettyTable<0.8,>=0.7.2\n",
      "  Downloading prettytable-0.7.2.tar.bz2 (21 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2020.6.20)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/site-packages (from networkx>=2.0->scikit-image) (4.4.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from jsonschema->ray) (1.7.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/site-packages (from jsonschema->ray) (20.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib64/python3.6/site-packages (from jsonschema->ray) (0.16.0)\n",
      "Collecting opencensus-context==0.1.1\n",
      "  Downloading opencensus_context-0.1.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting google-api-core<2.0.0,>=1.0.0\n",
      "  Downloading google_api_core-1.22.1-py2.py3-none-any.whl (91 kB)\n",
      "\u001b[K     |████████████████████████████████| 91 kB 14.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/lib/python3.6/site-packages (from requests->ray) (2.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests->ray) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->ray) (1.25.10)\n",
      "Collecting async-timeout\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting hiredis\n",
      "  Downloading hiredis-1.1.0-cp36-cp36m-manylinux2010_x86_64.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 8.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-ml-py3>=7.352.0\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Collecting psutil\n",
      "  Downloading psutil-5.7.2.tar.gz (460 kB)\n",
      "\u001b[K     |████████████████████████████████| 460 kB 23.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blessings>=1.6\n",
      "  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/site-packages (from google->ray) (4.9.1)\n",
      "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
      "Collecting multidict<5.0,>=4.5\n",
      "  Downloading multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 46.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=3.6.5; python_version < \"3.7\"\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.5.1-cp36-cp36m-manylinux1_x86_64.whl (257 kB)\n",
      "\u001b[K     |████████████████████████████████| 257 kB 25.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib64/python3.6/site-packages (from Mako->alembic->optuna) (1.1.1)\n",
      "Collecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.0.tar.gz (16 kB)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema->ray) (3.1.0)\n",
      "Collecting contextvars; python_version >= \"3.6\" and python_version < \"3.7\"\n",
      "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.19.1 in /usr/local/lib/python3.6/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.20.1)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 13.8 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.6/site-packages (from beautifulsoup4->google->ray) (2.0.1)\n",
      "Collecting immutables>=0.9\n",
      "  Downloading immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.19.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.19.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.19.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2.0dev,>=1.19.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.4.8)\n",
      "Building wheels for collected packages: optuna, alembic, gpustat, PrettyTable, nvidia-ml-py3, psutil, idna-ssl, pyperclip, contextvars\n",
      "  Building wheel for optuna (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optuna: filename=optuna-2.0.0-py3-none-any.whl size=312964 sha256=0287975d0cde5d65fe16ec05f0e042bc070518599eba35832e96fb53e21cf854\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/1f/c0/e68cc4a2c4088c8498110b5af910a0fe2679c758d20ea5a6fd\n",
      "  Building wheel for alembic (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for alembic: filename=alembic-1.4.2-py2.py3-none-any.whl size=159540 sha256=33b690ccba880e473d802e8860979c61b9d26353f8e9aaac5262fa978998f878\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/50/61/5cc491b0ca39be60dfb4dce940b389ff91b847d62e0eb2d680\n",
      "  Building wheel for gpustat (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=62c29b54a8b1d1a3f1ffd2f790933b1fb81b481e9286f6287def037a94767d5e\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/da/35/fe2cfb3bc47822299f5e124a599d56f00b30ec0b328db16b9f\n",
      "  Building wheel for PrettyTable (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PrettyTable: filename=prettytable-0.7.2-py3-none-any.whl size=13700 sha256=d0a4def874fce1c5691eac8e85989e30c7c7483055327341a96c63a3b5917f11\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/15/c3/5f28b42ae9c81638570b8b7ed654e0f98c5fdc08875869511b\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19191 sha256=0d2d7aaa6333102f026fa25a72166288d43fc91e798ee05a2f5825ed1236b1bc\n",
      "  Stored in directory: /root/.cache/pip/wheels/7f/26/a3/33f2079871e2bebb3f53a2b21c3ec64129b8efdd18a6263a52\n",
      "  Building wheel for psutil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psutil: filename=psutil-5.7.2-cp36-cp36m-linux_x86_64.whl size=266564 sha256=c5f4424554368d55e55efbe77560270ebebfe29f54bbcf67a46ca077391df892\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/1f/18/c066f071b2c2b638e40889c5595bc6b61fbe45eb475d29d61d\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3161 sha256=18ee50674cc2db9914ab9f9715d22ca4ecc1a45cb0c551ecfbcee44c0eb947ff\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.8.0-py3-none-any.whl size=8692 sha256=c4bf4d060537fe0738a2492dd00da3346c1d3196952932280b2e5c93a465c27d\n",
      "  Stored in directory: /root/.cache/pip/wheels/26/30/fe/92e2d4b1301ba74c07ea09c9e4c08f5bf12bae9c30319d74c5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for contextvars (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7665 sha256=a976d3b961ddeeb86c7d7b1a9b0b61c19811f46351b13237238a63c8690151a9\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/11/53/911724983aa48deb94792432e14e518447212dd6c5477d49d3\n",
      "Successfully built optuna alembic gpustat PrettyTable nvidia-ml-py3 psutil idna-ssl pyperclip contextvars\n",
      "Installing collected packages: keras-applications, astor, mock, tensorflow-estimator, tensorboard, tensorflow, Mako, python-editor, sqlalchemy, alembic, pbr, pyperclip, colorama, cmd2, stevedore, PrettyTable, cliff, cmaes, colorlog, tqdm, optuna, kiwisolver, cycler, matplotlib, imageio, tifffile, networkx, PyWavelets, scikit-image, redis, immutables, contextvars, opencensus-context, googleapis-common-protos, google-api-core, opencensus, msgpack, colorful, click, async-timeout, hiredis, aioredis, py-spy, nvidia-ml-py3, psutil, blessings, gpustat, google, idna-ssl, multidict, typing-extensions, yarl, aiohttp, ray\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.3.0\n",
      "    Uninstalling tensorboard-2.3.0:\n",
      "      Successfully uninstalled tensorboard-2.3.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.3.0\n",
      "    Uninstalling tensorflow-2.3.0:\n",
      "      Successfully uninstalled tensorflow-2.3.0\n",
      "Successfully installed Mako-1.1.3 PrettyTable-0.7.2 PyWavelets-1.1.1 aiohttp-3.6.2 aioredis-1.3.1 alembic-1.4.2 astor-0.8.1 async-timeout-3.0.1 blessings-1.7 click-7.1.2 cliff-3.4.0 cmaes-0.6.0 cmd2-1.3.5 colorama-0.4.3 colorful-0.5.4 colorlog-4.2.1 contextvars-2.4 cycler-0.10.0 google-3.0.0 google-api-core-1.22.1 googleapis-common-protos-1.52.0 gpustat-0.6.0 hiredis-1.1.0 idna-ssl-1.1.0 imageio-2.9.0 immutables-0.14 keras-applications-1.0.8 kiwisolver-1.2.0 matplotlib-3.3.1 mock-4.0.2 msgpack-1.0.0 multidict-4.7.6 networkx-2.5 nvidia-ml-py3-7.352.0 opencensus-0.7.10 opencensus-context-0.1.1 optuna-2.0.0 pbr-5.4.5 psutil-5.7.2 py-spy-0.3.3 pyperclip-1.8.0 python-editor-1.0.4 ray-0.8.7 redis-3.4.1 scikit-image-0.17.2 sqlalchemy-1.3.19 stevedore-3.2.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 tifffile-2020.8.25 tqdm-4.48.2 typing-extensions-3.7.4.3 yarl-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install keras==2.1.5 tensorflow==1.13.1 numpy pandas pillow sklearn optuna scikit-image ray ray[tune] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optkeras\n",
      "  Downloading optkeras-0.0.7-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: optuna>=0.9.0 in /usr/local/lib/python3.6/site-packages (from optkeras) (2.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib64/python3.6/site-packages (from optkeras) (1.19.1)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/site-packages (from optkeras) (2.1.5)\n",
      "Requirement already satisfied: cliff in /usr/local/lib/python3.6/site-packages (from optuna>=0.9.0->optkeras) (3.4.0)\n",
      "Requirement already satisfied: cmaes>=0.5.1 in /usr/local/lib/python3.6/site-packages (from optuna>=0.9.0->optkeras) (0.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/site-packages (from optuna>=0.9.0->optkeras) (20.4)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.6/site-packages (from optuna>=0.9.0->optkeras) (1.4.2)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib64/python3.6/site-packages (from optuna>=0.9.0->optkeras) (1.5.2)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.6/site-packages (from optuna>=0.9.0->optkeras) (4.2.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib64/python3.6/site-packages (from optuna>=0.9.0->optkeras) (1.3.19)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/site-packages (from optuna>=0.9.0->optkeras) (0.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/site-packages (from optuna>=0.9.0->optkeras) (4.48.2)\n",
      "Requirement already satisfied: pyyaml in /usr/lib64/python3.6/site-packages (from keras->optkeras) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3.6/site-packages (from keras->optkeras) (1.14.0)\n",
      "Requirement already satisfied: cmd2!=0.8.3,>=0.8.0 in /usr/local/lib/python3.6/site-packages (from cliff->optuna>=0.9.0->optkeras) (1.3.5)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.6/site-packages (from cliff->optuna>=0.9.0->optkeras) (3.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/site-packages (from cliff->optuna>=0.9.0->optkeras) (2.4.7)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/site-packages (from cliff->optuna>=0.9.0->optkeras) (5.4.5)\n",
      "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/site-packages (from cliff->optuna>=0.9.0->optkeras) (0.7.2)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.6/site-packages (from alembic->optuna>=0.9.0->optkeras) (1.1.3)\n",
      "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/site-packages (from alembic->optuna>=0.9.0->optkeras) (1.0.4)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/site-packages (from alembic->optuna>=0.9.0->optkeras) (2.8.1)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna>=0.9.0->optkeras) (20.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna>=0.9.0->optkeras) (1.7.0)\n",
      "Requirement already satisfied: setuptools>=34.4 in /usr/lib/python3.6/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna>=0.9.0->optkeras) (39.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna>=0.9.0->optkeras) (0.2.5)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.6/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna>=0.9.0->optkeras) (1.8.0)\n",
      "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.6/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna>=0.9.0->optkeras) (0.4.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib64/python3.6/site-packages (from Mako->alembic->optuna>=0.9.0->optkeras) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/site-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2!=0.8.3,>=0.8.0->cliff->optuna>=0.9.0->optkeras) (3.1.0)\n",
      "Installing collected packages: optkeras\n",
      "Successfully installed optkeras-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install optkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib64/python3.6/site-packages (1.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas) (2.8.1)\r\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.6/site-packages (from pandas) (1.19.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas) (2020.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\r\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Pegasus.api.workflow.Workflow at 0x7fb4869abf98>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import requests \n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "#Import Pegasus API\n",
    "from Pegasus.api import *\n",
    "\n",
    "\n",
    "#Replica Catalog\n",
    "rc = ReplicaCatalog()\n",
    "input_files = glob('*.jpg')\n",
    "input_files.sort()\n",
    "in_files=[]\n",
    "\n",
    "for file in input_files:\n",
    "    in_files.append(File(file))\n",
    "    rc.add_replica(\"local\", File(file), str(Path(\".\").resolve() / file))\n",
    "    \n",
    "rc.write()\n",
    "\n",
    "\n",
    "#Transformation\n",
    "pre_process_resize = Transformation( \"preprocess1.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/preprocess1.py\",\n",
    "        is_stageable=True)\n",
    "\n",
    "pre_process_augment = Transformation( \"Augmentation.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/Augmentation.py\",\n",
    "        is_stageable=True)\n",
    "\n",
    "data_split  = Transformation( \"Data_Split.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/Data_Split.py\",\n",
    "        is_stageable=True)\n",
    "\n",
    "vgg_model  = Transformation( \"VGG_model.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/VGG_model.py\",\n",
    "        is_stageable=True)\n",
    "\n",
    "hpo =  Transformation( \"Optuna.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/Optuna.py\",\n",
    "        is_stageable=True)\n",
    "                    \n",
    "tc = TransformationCatalog()\\\n",
    "    .add_transformations(pre_process_resize,pre_process_augment,data_split,vgg_model,hpo)\\\n",
    "    .write()\n",
    "\n",
    "#Workflow\n",
    "wf = Workflow(\"Cats_and_Dogs\", infer_dependencies=True)\n",
    "\n",
    "\n",
    "resized_images = File('resized_images.txt')\n",
    "all_files = [File(\"resized_{}\".format(f.lfn)) for f in in_files]\n",
    "labels = File('labels.txt')\n",
    "\n",
    "job_preprocess1 = Job(pre_process_resize)\\\n",
    "                    .add_inputs(*in_files)\\\n",
    "                    .add_outputs(*all_files,resized_images,labels) \n",
    "\n",
    "aug_images_txt = File('augmentation.txt')\n",
    "aug_labels_txt = File('aug_labels.txt')\n",
    "augmented_files = []\n",
    "for f in all_files:\n",
    "    augmented_files.extend([File(str(f).replace(\"{}\".format(os.path.splitext(str(f))[0]), \"Aug_{}_{}\".format(os.path.splitext(str(f))[0],i))) for i in range(3)])\n",
    "\n",
    "    \n",
    "job_preprocess2 = Job(pre_process_augment)\\\n",
    "                    .add_inputs(*all_files,labels)\\\n",
    "                    .add_outputs(aug_images_txt,aug_labels_txt,*augmented_files)\n",
    "\n",
    "training_data = File('training.pkl')\n",
    "testing_data = File('testing.pkl')\n",
    "val_data = File('validation.pkl')\n",
    "\n",
    "job_data_split = Job(data_split)\\\n",
    "                    .add_inputs(*augmented_files,labels)\\\n",
    "                    .add_outputs(training_data,testing_data,val_data)\n",
    "\n",
    "model = File('model.h5')\n",
    "\n",
    "job_vgg_model = Job(vgg_model)\\\n",
    "                    .add_inputs(*augmented_files,training_data,testing_data,val_data)\\\n",
    "                    .add_outputs(model)\n",
    "model1 = File('model_hpo.h5')\n",
    "optuna_csv = File(\"Optuna_log_file.csv\")\n",
    "\n",
    "job_hpo = Job(hpo)\\\n",
    "                    .add_args(optuna_csv)\\\n",
    "                    .add_inputs(model,*augmented_files,training_data,testing_data,val_data)\\\n",
    "                    .add_outputs(model1,optuna_csv)\n",
    "\n",
    "\n",
    "wf.add_jobs(job_preprocess1,job_preprocess2,job_data_split,job_vgg_model,job_hpo)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "################\n",
      "# pegasus-plan #\n",
      "################\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword $defs - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword additionalItems - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword examples - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "2020.08.27 03:17:21.054 UTC:\n",
      "2020.08.27 03:17:21.060 UTC:   -----------------------------------------------------------------------\n",
      "2020.08.27 03:17:21.065 UTC:   File for submitting this DAG to HTCondor           : Cats_and_Dogs-0.dag.condor.sub\n",
      "2020.08.27 03:17:21.071 UTC:   Log of DAGMan debugging messages                 : Cats_and_Dogs-0.dag.dagman.out\n",
      "2020.08.27 03:17:21.076 UTC:   Log of HTCondor library output                     : Cats_and_Dogs-0.dag.lib.out\n",
      "2020.08.27 03:17:21.082 UTC:   Log of HTCondor library error messages             : Cats_and_Dogs-0.dag.lib.err\n",
      "2020.08.27 03:17:21.087 UTC:   Log of the life of condor_dagman itself          : Cats_and_Dogs-0.dag.dagman.log\n",
      "2020.08.27 03:17:21.093 UTC:\n",
      "2020.08.27 03:17:21.099 UTC:   -no_submit given, not submitting DAG to HTCondor.  You can do this with:\n",
      "2020.08.27 03:17:21.111 UTC:   -----------------------------------------------------------------------\n",
      "2020.08.27 03:17:22.064 UTC:   Your database is compatible with Pegasus version: 5.0.0dev\n",
      "2020.08.27 03:17:23.390 UTC:   Created Pegasus database in: sqlite:////home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0021/Cats_and_Dogs-0.replicas.db\n",
      "2020.08.27 03:17:23.395 UTC:   Your database is compatible with Pegasus version: 5.0.0dev\n",
      "2020.08.27 03:17:23.462 UTC:   Output replica catalog set to jdbc:sqlite:/home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0021/Cats_and_Dogs-0.replicas.db\n",
      "2020.08.27 03:17:23.669 UTC:   Submitting to condor Cats_and_Dogs-0.dag.condor.sub\n",
      "2020.08.27 03:17:23.701 UTC:\n",
      "2020.08.27 03:17:23.707 UTC:   Your workflow has been started and is running in the base directory:\n",
      "2020.08.27 03:17:23.713 UTC:\n",
      "2020.08.27 03:17:23.718 UTC:   /home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0021\n",
      "2020.08.27 03:17:23.724 UTC:\n",
      "2020.08.27 03:17:23.729 UTC:   *** To monitor the workflow you can run ***\n",
      "2020.08.27 03:17:23.735 UTC:\n",
      "2020.08.27 03:17:23.740 UTC:   pegasus-status -l /home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0021\n",
      "2020.08.27 03:17:23.746 UTC:\n",
      "2020.08.27 03:17:23.752 UTC:   *** To remove your workflow run ***\n",
      "2020.08.27 03:17:23.757 UTC:\n",
      "2020.08.27 03:17:23.763 UTC:   pegasus-remove /home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0021\n",
      "2020.08.27 03:17:24.640 UTC:   Time taken to execute is 5.314 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;32m##################################################\u001b[0m] 100.0% ..Success (\u001b[1;32mCompleted: 29\u001b[0m, \u001b[1;33mQueued: 0\u001b[0m, \u001b[1;36mRunning: 0\u001b[0m, \u001b[1;31mFailed: 0\u001b[0m)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "####################\n",
      "# pegasus-analyzer #\n",
      "####################\n",
      "Your database is compatible with Pegasus version: 5.0.0dev\n",
      "\n",
      "************************************Summary*************************************\n",
      "\n",
      "Submit Directory   : /home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0021\n",
      "Total jobs         :     29 (100.00%)\n",
      "# jobs succeeded   :     29 (100.00%)\n",
      "# jobs failed      :      0 (0.00%)\n",
      "# jobs held        :      0 (0.00%)\n",
      "# jobs unsubmitted :      0 (0.00%)\n",
      "\n",
      "\n",
      "\n",
      "######################\n",
      "# pegasus-statistics #\n",
      "######################\n",
      "Your database is compatible with Pegasus version: 5.0.0dev\n",
      "\n",
      "#\n",
      "# Pegasus Workflow Management System - http://pegasus.isi.edu\n",
      "#\n",
      "# Workflow summary:\n",
      "#   Summary of the workflow execution. It shows total\n",
      "#   tasks/jobs/sub workflows run, how many succeeded/failed etc.\n",
      "#   In case of hierarchical workflow the calculation shows the\n",
      "#   statistics across all the sub workflows.It shows the following\n",
      "#   statistics about tasks, jobs and sub workflows.\n",
      "#     * Succeeded - total count of succeeded tasks/jobs/sub workflows.\n",
      "#     * Failed - total count of failed tasks/jobs/sub workflows.\n",
      "#     * Incomplete - total count of tasks/jobs/sub workflows that are\n",
      "#       not in succeeded or failed state. This includes all the jobs\n",
      "#       that are not submitted, submitted but not completed etc. This\n",
      "#       is calculated as  difference between 'total' count and sum of\n",
      "#       'succeeded' and 'failed' count.\n",
      "#     * Total - total count of tasks/jobs/sub workflows.\n",
      "#     * Retries - total retry count of tasks/jobs/sub workflows.\n",
      "#     * Total+Retries - total count of tasks/jobs/sub workflows executed\n",
      "#       during workflow run. This is the cumulative of retries,\n",
      "#       succeeded and failed count.\n",
      "# Workflow wall time:\n",
      "#   The wall time from the start of the workflow execution to the end as\n",
      "#   reported by the DAGMAN.In case of rescue dag the value is the\n",
      "#   cumulative of all retries.\n",
      "# Cumulative job wall time:\n",
      "#   The sum of the wall time of all jobs as reported by kickstart.\n",
      "#   In case of job retries the value is the cumulative of all retries.\n",
      "#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),\n",
      "#   the wall time value includes jobs from the sub workflows as well.\n",
      "# Cumulative job wall time as seen from submit side:\n",
      "#   The sum of the wall time of all jobs as reported by DAGMan.\n",
      "#   This is similar to the regular cumulative job wall time, but includes\n",
      "#   job management overhead and delays. In case of job retries the value\n",
      "#   is the cumulative of all retries. For workflows having sub workflow\n",
      "#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs\n",
      "#   from the sub workflows as well.\n",
      "# Cumulative job badput wall time:\n",
      "#   The sum of the wall time of all failed jobs as reported by kickstart.\n",
      "#   In case of job retries the value is the cumulative of all retries.\n",
      "#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),\n",
      "#   the wall time value includes jobs from the sub workflows as well.\n",
      "# Cumulative job badput wall time as seen from submit side:\n",
      "#   The sum of the wall time of all failed jobs as reported by DAGMan.\n",
      "#   This is similar to the regular cumulative job badput wall time, but includes\n",
      "#   job management overhead and delays. In case of job retries the value\n",
      "#   is the cumulative of all retries. For workflows having sub workflow\n",
      "#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs\n",
      "#   from the sub workflows as well.\n",
      "------------------------------------------------------------------------------\n",
      "Type           Succeeded Failed  Incomplete  Total     Retries   Total+Retries\n",
      "Tasks          5         0       0           5         0         5\n",
      "Jobs           29        0       0           29        0         29\n",
      "Sub-Workflows  0         0       0           0         0         0\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Workflow wall time                                       : 43 mins, 22 secs\n",
      "Cumulative job wall time                                 : 43 mins, 32 secs\n",
      "Cumulative job wall time as seen from submit side        : 44 mins, 0 secs\n",
      "Cumulative job badput wall time                          : 0.0 secs\n",
      "Cumulative job badput wall time as seen from submit side : 0.0 secs\n",
      "\n",
      "# Integrity Metrics\n",
      "# Number of files for which checksums were compared/computed along with total time spent doing it.\n",
      "3024 files checksums compared with total duration of 2 mins, 50 secs\n",
      "1015 files checksums generated with total duration of 4.74 secs\n",
      "\n",
      "# Integrity Errors\n",
      "# Total:\n",
      "#       Total number of integrity errors encountered across all job executions(including retries) of a workflow.\n",
      "# Failures:\n",
      "#       Number of failed jobs where the last job instance had integrity errors.\n",
      "Failures: 0 job failures had integrity errors\n",
      "\n",
      "Summary                       : /home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0021/statistics/summary.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "     wf.plan(submit=True)\\\n",
    "        .wait()\\\n",
    "        .analyze()\\\n",
    "        .statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "         \n",
    "\n",
    "\n",
    "aug_images_txt = File('augmentation.txt')\n",
    "aug_labels_txt = File('aug_labels.txt')\n",
    "\n",
    "job_preprocess2 = Job(pre_process_augment)\\\n",
    "                    .add_inputs(labels,resized_images)\\\n",
    "                    .add_outputs(aug_images_txt,aug_labels_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_files = []\n",
    "for f in all_files:\n",
    "    augmented_files.extend([File(str(f).replace(\"{}\".format(os.path.splitext(str(f))[0]), \"Aug_{}_{}\".format(os.path.splitext(str(f))[0],i))) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(augmented_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pre_process_augment = Transformation( \"Augmentation.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/Augmentation.py\",\n",
    "        is_stageable=True)   \n",
    "\n",
    "\n",
    "aug_images_txt = File('augmentation.txt')\n",
    "aug_labels_txt = File('aug_labels.txt')\n",
    "augmented_files = []\n",
    "for f in all_files:\n",
    "    augmented_files.extend([File(str(f).replace(\"{}\".format(os.path.splitext(str(f))[0]), \"Aug_{}_{}\".format(os.path.splitext(str(f))[0],i))) for i in range(3)])\n",
    "\n",
    "job_preprocess2 = Job(pre_process_augment)\\\n",
    "                    .add_inputs(*all_files,labels,resized_images)\\\n",
    "                    .add_outputs(aug_images_txt,aug_labels_txt,*augmented_files)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
