{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import requests from optkeras.optkeras import OptKeras\n",
    "import optkeras\n",
    "import pickle\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input,Dense,BatchNormalization,Flatten,Dropout,GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import layer_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model,load_model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import optuna\n",
    "from zipfile import ZipFile\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "#Import Pegasus API\n",
    "from Pegasus.api import *\n",
    "\n",
    "#Downloading dataset\n",
    "zipurl = 'https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip'    \n",
    "r = requests.get(zipurl, stream = True) \n",
    "  \n",
    "with open(\"dataset.zip\",\"wb\") as pdf: \n",
    "    for chunk in r.iter_content(chunk_size=1024): \n",
    "         if chunk: \n",
    "            pdf.write(chunk)\n",
    "            \n",
    "# Extract all the contents of zip file in current directory            \n",
    "with ZipFile('dataset.zip', 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optkeras.optkeras import OptKeras\n",
    "import optkeras\n",
    "import pickle\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input,Dense,BatchNormalization,Flatten,Dropout,GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import layer_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model,load_model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip3 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo pip3 install  keras==2.1.5 tensorflow numpy pandas pillow opencv-python sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo pip3 install keras==2.1.5 tensorflow==1.13.1 numpy pandas pillow sklearn optuna scikit-image ray ray[tune] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo pip3 install optkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Pegasus.api.workflow.Workflow at 0x7fa24a039c88>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import requests \n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "#Import Pegasus API\n",
    "from Pegasus.api import *\n",
    "\n",
    "\n",
    "#Replica Catalog\n",
    "rc = ReplicaCatalog()\n",
    "input_files = glob('*.jpg')\n",
    "input_files.sort()\n",
    "in_files=[]\n",
    "\n",
    "for file in input_files:\n",
    "    in_files.append(File(file))\n",
    "    rc.add_replica(\"local\", File(file), str(Path(\".\").resolve() / file))\n",
    "    \n",
    "rc.write()\n",
    "\n",
    "\n",
    "#Transformation\n",
    "pre_process_resize = Transformation( \"preprocess1.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/preprocess1.py\",\n",
    "        is_stageable=True)\n",
    "\n",
    "pre_process_augment = Transformation( \"Augmentation.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/Augmentation.py\",\n",
    "        is_stageable=True)\n",
    "\n",
    "data_split  = Transformation( \"Data_Split.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/Data_Split.py\",\n",
    "        is_stageable=True)\n",
    "\n",
    "vgg_model  = Transformation( \"VGG_model.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/VGG_model.py\",\n",
    "        is_stageable=True)\n",
    "\n",
    "hpo =  Transformation( \"Optuna.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/Optuna.py\",\n",
    "        is_stageable=True)\n",
    "                    \n",
    "tc = TransformationCatalog()\\\n",
    "    .add_transformations(pre_process_resize,pre_process_augment,data_split,vgg_model,hpo)\\\n",
    "    .write()\n",
    "\n",
    "#Workflow\n",
    "wf = Workflow(\"Cats_and_Dogs\", infer_dependencies=True)\n",
    "\n",
    "\n",
    "resized_images = File('resized_images.txt')\n",
    "all_files = [File(\"resized_{}\".format(f.lfn)) for f in in_files]\n",
    "labels = File('labels.txt')\n",
    "\n",
    "job_preprocess1 = Job(pre_process_resize)\\\n",
    "                    .add_inputs(*in_files)\\\n",
    "                    .add_outputs(*all_files,resized_images,labels) \n",
    "\n",
    "aug_images_txt = File('augmentation.txt')\n",
    "aug_labels_txt = File('aug_labels.txt')\n",
    "augmented_files = []\n",
    "for f in all_files:\n",
    "    augmented_files.extend([File(str(f).replace(\"{}\".format(os.path.splitext(str(f))[0]), \"Aug_{}_{}\".format(os.path.splitext(str(f))[0],i))) for i in range(3)])\n",
    "\n",
    "    \n",
    "job_preprocess2 = Job(pre_process_augment)\\\n",
    "                    .add_inputs(*all_files,labels)\\\n",
    "                    .add_outputs(aug_images_txt,aug_labels_txt,*augmented_files)\n",
    "\n",
    "training_data = File('training.pkl')\n",
    "testing_data = File('testing.pkl')\n",
    "val_data = File('validation.pkl')\n",
    "\n",
    "job_data_split = Job(data_split)\\\n",
    "                    .add_inputs(*augmented_files,labels)\\\n",
    "                    .add_outputs(training_data,testing_data,val_data)\n",
    "\n",
    "model = File('model.h5')\n",
    "\n",
    "job_vgg_model = Job(vgg_model)\\\n",
    "                    .add_inputs(*augmented_files,training_data,testing_data,val_data)\\\n",
    "                    .add_outputs(model)\n",
    "model1 = File('model_hpo.h5')\n",
    "optuna_csv = File(\"Optuna_log_file.csv\")\n",
    "\n",
    "job_hpo = Job(hpo)\\\n",
    "                    .add_args(optuna_csv)\\\n",
    "                    .add_inputs(model,*augmented_files,training_data,testing_data,val_data)\\\n",
    "                    .add_outputs(model1,optuna_csv)\n",
    "\n",
    "\n",
    "wf.add_jobs(job_preprocess1,job_preprocess2,job_data_split,job_vgg_model,job_hpo)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "     wf.plan(submit=True)\\\n",
    "        .wait()\\\n",
    "        .analyze()\\\n",
    "        .statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "         \n",
    "\n",
    "\n",
    "aug_images_txt = File('augmentation.txt')\n",
    "aug_labels_txt = File('aug_labels.txt')\n",
    "\n",
    "job_preprocess2 = Job(pre_process_augment)\\\n",
    "                    .add_inputs(labels,resized_images)\\\n",
    "                    .add_outputs(aug_images_txt,aug_labels_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_files = []\n",
    "for f in all_files:\n",
    "    augmented_files.extend([File(str(f).replace(\"{}\".format(os.path.splitext(str(f))[0]), \"Aug_{}_{}\".format(os.path.splitext(str(f))[0],i))) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(augmented_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pre_process_augment = Transformation( \"Augmentation.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/Augmentation.py\",\n",
    "        is_stageable=True)   \n",
    "\n",
    "\n",
    "aug_images_txt = File('augmentation.txt')\n",
    "aug_labels_txt = File('aug_labels.txt')\n",
    "augmented_files = []\n",
    "for f in all_files:\n",
    "    augmented_files.extend([File(str(f).replace(\"{}\".format(os.path.splitext(str(f))[0]), \"Aug_{}_{}\".format(os.path.splitext(str(f))[0],i))) for i in range(3)])\n",
    "\n",
    "job_preprocess2 = Job(pre_process_augment)\\\n",
    "                    .add_inputs(*all_files,labels,resized_images)\\\n",
    "                    .add_outputs(aug_images_txt,aug_labels_txt,*augmented_files)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
