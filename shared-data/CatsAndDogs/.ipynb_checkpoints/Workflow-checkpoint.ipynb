{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-20.2.3-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras==2.1.5\n",
      "  Downloading Keras-2.1.5-py2.py3-none-any.whl (334 kB)\n",
      "\u001b[K     |████████████████████████████████| 334 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow==1.13.1\n",
      "  Downloading tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 92.5 MB 198 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.19.2-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.1.2-cp36-cp36m-manylinux1_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow\n",
      "  Downloading Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting optuna\n",
      "  Downloading optuna-2.1.0.tar.gz (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 37.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-image\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting optkeras\n",
      "  Downloading optkeras-0.0.7-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/lib/python3.6/site-packages (from keras==2.1.5) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/lib64/python3.6/site-packages (from keras==2.1.5) (3.13)\n",
      "Collecting scipy>=0.14\n",
      "  Downloading scipy-1.5.2-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
      "\u001b[K     |████████████████████████████████| 367 kB 28.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.1\n",
      "  Downloading protobuf-3.13.0-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 35.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel>=0.26\n",
      "  Using cached wheel-0.35.1-py2.py3-none-any.whl (33 kB)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 4.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.1.6\n",
      "  Downloading absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n",
      "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 30.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas) (2020.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cliff\n",
      "  Downloading cliff-3.4.0-py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorlog\n",
      "  Downloading colorlog-4.2.1-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.6/site-packages (from optuna) (20.4)\n",
      "Collecting cmaes>=0.6.0\n",
      "  Downloading cmaes-0.6.1-py3-none-any.whl (9.7 kB)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.4.3-py2.py3-none-any.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlalchemy>=1.1.0\n",
      "  Downloading SQLAlchemy-1.3.19-cp36-cp36m-manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 40.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "\u001b[K     |████████████████████████████████| 300 kB 35.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.0\n",
      "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 28.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 9.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imageio>=2.3.0\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 30.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib!=3.0.0,>=2.0.0\n",
      "  Downloading matplotlib-3.3.2-cp36-cp36m-manylinux1_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 25.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mock>=2.0.0\n",
      "  Downloading mock-4.0.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (39.2.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 36.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting cmd2!=0.8.3,>=0.8.0\n",
      "  Downloading cmd2-1.3.10-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PrettyTable<0.8,>=0.7.2\n",
      "  Downloading prettytable-0.7.2.tar.bz2 (21 kB)\n",
      "Collecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.2.2-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.5.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 20.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing>=2.1.0 in /usr/local/lib/python3.6/site-packages (from cliff->optuna) (2.4.7)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.3-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/site-packages (from networkx>=2.0->scikit-image) (4.4.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 5.2 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2020.06.20 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.7.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.0.tar.gz (16 kB)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth>=0.1.7 in /usr/local/lib/python3.6/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=16.3.0 in /usr/local/lib/python3.6/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (20.1.0)\n",
      "Collecting colorama>=0.3.7\n",
      "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /usr/local/lib64/python3.6/site-packages (from Mako->alembic->optuna) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.0)\n",
      "Using legacy 'setup.py install' for sklearn, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for termcolor, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for PrettyTable, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for pyperclip, since package 'wheel' is not installed.\n",
      "Building wheels for collected packages: optuna\n",
      "  Building wheel for optuna (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optuna: filename=optuna-2.1.0-py3-none-any.whl size=321090 sha256=4cbe8319310dd81b99266205555f6391944f0c2359cb267a64e90c8a0b125cdb\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/13/e3/e2c767339ab685a3fae35e10741b5c4345369a901352cc8d5a\n",
      "Successfully built optuna\n",
      "Installing collected packages: pip, numpy, scipy, keras, mock, absl-py, tensorflow-estimator, protobuf, wheel, keras-preprocessing, markdown, grpcio, werkzeug, tensorboard, termcolor, h5py, keras-applications, gast, astor, tensorflow, pandas, pillow, threadpoolctl, joblib, scikit-learn, sklearn, pyperclip, colorama, cmd2, PrettyTable, pbr, stevedore, cliff, colorlog, cmaes, Mako, python-editor, sqlalchemy, alembic, tqdm, optuna, networkx, tifffile, imageio, kiwisolver, cycler, matplotlib, PyWavelets, scikit-image, optkeras\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.2\n",
      "    Uninstalling pip-20.2.2:\n",
      "      Successfully uninstalled pip-20.2.2\n",
      "    Running setup.py install for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for pyperclip ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for PrettyTable ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed Mako-1.1.3 PrettyTable-0.7.2 PyWavelets-1.1.1 absl-py-0.10.0 alembic-1.4.3 astor-0.8.1 cliff-3.4.0 cmaes-0.6.1 cmd2-1.3.10 colorama-0.4.3 colorlog-4.2.1 cycler-0.10.0 gast-0.4.0 grpcio-1.32.0 h5py-2.10.0 imageio-2.9.0 joblib-0.16.0 keras-2.1.5 keras-applications-1.0.8 keras-preprocessing-1.1.2 kiwisolver-1.2.0 markdown-3.2.2 matplotlib-3.3.2 mock-4.0.2 networkx-2.5 numpy-1.19.2 optkeras-0.0.7 optuna-2.1.0 pandas-1.1.2 pbr-5.5.0 pillow-7.2.0 pip-20.2.3 protobuf-3.13.0 pyperclip-1.8.0 python-editor-1.0.4 scikit-image-0.17.2 scikit-learn-0.23.2 scipy-1.5.2 sklearn-0.0 sqlalchemy-1.3.19 stevedore-3.2.2 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0 threadpoolctl-2.1.0 tifffile-2020.9.3 tqdm-4.49.0 werkzeug-1.0.1 wheel-0.35.1\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install --upgrade pip keras==2.1.5 tensorflow==1.13.1 numpy pandas pillow sklearn optuna scikit-image  optkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Pegasus.api.workflow.Workflow at 0x7fade4fbf198>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import requests \n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "#Import Pegasus API\n",
    "from Pegasus.api import *\n",
    "\n",
    "#Properties\n",
    "props = Properties()\n",
    "props[\"dagman.retry\"] = \"100\"\n",
    "props[\"pegasus.transfer.arguments\"] = \"-m 1\"\n",
    "props.write()\n",
    "\n",
    "#Replica Catalog\n",
    "rc = ReplicaCatalog()\n",
    "input_files = glob('*.jpg')\n",
    "input_files.sort()\n",
    "in_files=[]\n",
    "\n",
    "checkpoint_file = \"best_model.hdf5\"\n",
    "CHECKPOINT_FILE = \"saved_stated.txt\"\n",
    "number = 1\n",
    "\n",
    "with open(CHECKPOINT_FILE, 'w') as f:\n",
    "    f.write('%d' % number)\n",
    "\n",
    "\n",
    "for file in input_files:\n",
    "    in_files.append(File(file))\n",
    "    rc.add_replica(\"local\", File(file), str(Path(\".\").resolve() / file))  \n",
    "rc.add_replica(\"local\", File(\"best_model.hdf5\"), str(Path(\".\").resolve() / \"best_model.hdf5\")) \n",
    "rc.add_replica(\"local\", \"saved_stated.txt\", Path(\".\").resolve() / \"saved_stated.txt\")\\\n",
    "        .write()\n",
    "rc.write()\n",
    "\n",
    "\n",
    "#Transformation\n",
    "pre_process_resize = Transformation( \"preprocess1.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/preprocess1.py\",\n",
    "        is_stageable=True)\n",
    "\n",
    "pre_process_augment = Transformation( \"Augmentation.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/Augmentation.py\",\n",
    "        is_stageable=True)\n",
    "\n",
    "data_split  = Transformation( \"Data_Split.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/Data_Split.py\",\n",
    "        is_stageable=True)\n",
    "\n",
    "vgg_model  = Transformation( \"VGG_model.py\",\n",
    "        site = \"local\",\n",
    "        pfn = \"/home/scitech/shared-data/CatsAndDogs/VGG_model.py\",\n",
    "        is_stageable=True)\n",
    "\n",
    "# hpo =  Transformation( \"hpo_checkpointing.py\",\n",
    "#         site = \"local\",\n",
    "#         pfn=str(Path(\".\").resolve() /\"hpo_checkpointing.py\"),\n",
    "#         is_stageable=True)\n",
    "                    \n",
    "tc = TransformationCatalog()\\\n",
    "    .add_transformations(pre_process_resize,pre_process_augment,data_split,vgg_model)\\\n",
    "    .write()\n",
    "\n",
    "#Workflow\n",
    "wf = Workflow(\"Cats_and_Dogs\", infer_dependencies=True)\n",
    "\n",
    "\n",
    "resized_images = File('resized_images.txt')\n",
    "all_files = [File(\"resized_{}\".format(f.lfn)) for f in in_files]\n",
    "labels = File('labels.txt')\n",
    "\n",
    "job_preprocess1 = Job(pre_process_resize)\\\n",
    "                    .add_inputs(*in_files)\\\n",
    "                    .add_outputs(*all_files,resized_images,labels) \n",
    "\n",
    "aug_images_txt = File('augmentation.txt')\n",
    "aug_labels_txt = File('aug_labels.txt')\n",
    "augmented_files = []\n",
    "for f in all_files:\n",
    "    augmented_files.extend([File(str(f).replace(\"{}\".format(os.path.splitext(str(f))[0]), \"Aug_{}_{}\".format(os.path.splitext(str(f))[0],i))) for i in range(3)])\n",
    "\n",
    "    \n",
    "job_preprocess2 = Job(pre_process_augment)\\\n",
    "                    .add_inputs(*all_files,labels)\\\n",
    "                    .add_outputs(aug_images_txt,aug_labels_txt,*augmented_files)\n",
    "\n",
    "training_data = File('training.pkl')\n",
    "testing_data = File('testing.pkl')\n",
    "val_data = File('validation.pkl')\n",
    "\n",
    "job_data_split = Job(data_split)\\\n",
    "                    .add_inputs(*augmented_files,labels)\\\n",
    "                    .add_outputs(training_data,testing_data,val_data)\n",
    "\n",
    "model = File('model.h5')\n",
    "\n",
    "job_vgg_model = Job(vgg_model)\\\n",
    "                    .add_checkpoint(File(checkpoint_file),File(CHECKPOINT_FILE) stage_out=True)\\\n",
    "                    .add_inputs(*augmented_files,training_data,testing_data,val_data)\\\n",
    "                    .add_profiles(Namespace.PEGASUS, key=\"maxwalltime\", value=1)\\\n",
    "                    .add_outputs(model)\n",
    "\n",
    "# job_hpo = Job(hpo)\\\n",
    "#                     .add_checkpoint(File(checkpoint_file), stage_out=True)\\\n",
    "#                     .add_inputs(model,*augmented_files,training_data,testing_data,val_data)\\\n",
    "#                     .add_profiles(Namespace.PEGASUS, key=\"maxwalltime\", value=100)\n",
    "\n",
    "wf.add_jobs(job_preprocess1,job_preprocess2,job_data_split,job_vgg_model)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "################\n",
      "# pegasus-plan #\n",
      "################\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword $defs - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword additionalItems - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword examples - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "2020.09.27 18:27:36.966 UTC:\n",
      "2020.09.27 18:27:36.972 UTC:   -----------------------------------------------------------------------\n",
      "2020.09.27 18:27:36.977 UTC:   File for submitting this DAG to HTCondor           : Cats_and_Dogs-0.dag.condor.sub\n",
      "2020.09.27 18:27:36.983 UTC:   Log of DAGMan debugging messages                 : Cats_and_Dogs-0.dag.dagman.out\n",
      "2020.09.27 18:27:36.988 UTC:   Log of HTCondor library output                     : Cats_and_Dogs-0.dag.lib.out\n",
      "2020.09.27 18:27:36.994 UTC:   Log of HTCondor library error messages             : Cats_and_Dogs-0.dag.lib.err\n",
      "2020.09.27 18:27:36.999 UTC:   Log of the life of condor_dagman itself          : Cats_and_Dogs-0.dag.dagman.log\n",
      "2020.09.27 18:27:37.005 UTC:\n",
      "2020.09.27 18:27:37.010 UTC:   -no_submit given, not submitting DAG to HTCondor.  You can do this with:\n",
      "2020.09.27 18:27:37.021 UTC:   -----------------------------------------------------------------------\n",
      "2020.09.27 18:27:38.065 UTC:   Your database is compatible with Pegasus version: 5.0.0dev\n",
      "2020.09.27 18:27:39.345 UTC:   Created Pegasus database in: sqlite:////home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0029/Cats_and_Dogs-0.replicas.db\n",
      "2020.09.27 18:27:39.350 UTC:   Your database is compatible with Pegasus version: 5.0.0dev\n",
      "2020.09.27 18:27:39.402 UTC:   Output replica catalog set to jdbc:sqlite:/home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0029/Cats_and_Dogs-0.replicas.db\n",
      "2020.09.27 18:27:39.648 UTC:   Submitting to condor Cats_and_Dogs-0.dag.condor.sub\n",
      "2020.09.27 18:27:39.682 UTC:\n",
      "2020.09.27 18:27:39.688 UTC:   Your workflow has been started and is running in the base directory:\n",
      "2020.09.27 18:27:39.694 UTC:\n",
      "2020.09.27 18:27:39.699 UTC:   /home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0029\n",
      "2020.09.27 18:27:39.705 UTC:\n",
      "2020.09.27 18:27:39.711 UTC:   *** To monitor the workflow you can run ***\n",
      "2020.09.27 18:27:39.716 UTC:\n",
      "2020.09.27 18:27:39.722 UTC:   pegasus-status -l /home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0029\n",
      "2020.09.27 18:27:39.727 UTC:\n",
      "2020.09.27 18:27:39.733 UTC:   *** To remove your workflow run ***\n",
      "2020.09.27 18:27:39.739 UTC:\n",
      "2020.09.27 18:27:39.744 UTC:   pegasus-remove /home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0029\n",
      "2020.09.27 18:27:40.605 UTC:   Time taken to execute is 4.82 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;32m##################################################\u001b[0m] 100.0% ..Success (\u001b[1;32mCompleted: 25\u001b[0m, \u001b[1;33mQueued: 0\u001b[0m, \u001b[1;36mRunning: 0\u001b[0m, \u001b[1;31mFailed: 0\u001b[0m)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "####################\n",
      "# pegasus-analyzer #\n",
      "####################\n",
      "Your database is compatible with Pegasus version: 5.0.0dev\n",
      "\n",
      "************************************Summary*************************************\n",
      "\n",
      "Submit Directory   : /home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0029\n",
      "Total jobs         :     25 (100.00%)\n",
      "# jobs succeeded   :     25 (100.00%)\n",
      "# jobs failed      :      0 (0.00%)\n",
      "# jobs held        :      0 (0.00%)\n",
      "# jobs unsubmitted :      0 (0.00%)\n",
      "\n",
      "\n",
      "\n",
      "######################\n",
      "# pegasus-statistics #\n",
      "######################\n",
      "Your database is compatible with Pegasus version: 5.0.0dev\n",
      "\n",
      "#\n",
      "# Pegasus Workflow Management System - http://pegasus.isi.edu\n",
      "#\n",
      "# Workflow summary:\n",
      "#   Summary of the workflow execution. It shows total\n",
      "#   tasks/jobs/sub workflows run, how many succeeded/failed etc.\n",
      "#   In case of hierarchical workflow the calculation shows the\n",
      "#   statistics across all the sub workflows.It shows the following\n",
      "#   statistics about tasks, jobs and sub workflows.\n",
      "#     * Succeeded - total count of succeeded tasks/jobs/sub workflows.\n",
      "#     * Failed - total count of failed tasks/jobs/sub workflows.\n",
      "#     * Incomplete - total count of tasks/jobs/sub workflows that are\n",
      "#       not in succeeded or failed state. This includes all the jobs\n",
      "#       that are not submitted, submitted but not completed etc. This\n",
      "#       is calculated as  difference between 'total' count and sum of\n",
      "#       'succeeded' and 'failed' count.\n",
      "#     * Total - total count of tasks/jobs/sub workflows.\n",
      "#     * Retries - total retry count of tasks/jobs/sub workflows.\n",
      "#     * Total+Retries - total count of tasks/jobs/sub workflows executed\n",
      "#       during workflow run. This is the cumulative of retries,\n",
      "#       succeeded and failed count.\n",
      "# Workflow wall time:\n",
      "#   The wall time from the start of the workflow execution to the end as\n",
      "#   reported by the DAGMAN.In case of rescue dag the value is the\n",
      "#   cumulative of all retries.\n",
      "# Cumulative job wall time:\n",
      "#   The sum of the wall time of all jobs as reported by kickstart.\n",
      "#   In case of job retries the value is the cumulative of all retries.\n",
      "#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),\n",
      "#   the wall time value includes jobs from the sub workflows as well.\n",
      "# Cumulative job wall time as seen from submit side:\n",
      "#   The sum of the wall time of all jobs as reported by DAGMan.\n",
      "#   This is similar to the regular cumulative job wall time, but includes\n",
      "#   job management overhead and delays. In case of job retries the value\n",
      "#   is the cumulative of all retries. For workflows having sub workflow\n",
      "#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs\n",
      "#   from the sub workflows as well.\n",
      "# Cumulative job badput wall time:\n",
      "#   The sum of the wall time of all failed jobs as reported by kickstart.\n",
      "#   In case of job retries the value is the cumulative of all retries.\n",
      "#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),\n",
      "#   the wall time value includes jobs from the sub workflows as well.\n",
      "# Cumulative job badput wall time as seen from submit side:\n",
      "#   The sum of the wall time of all failed jobs as reported by DAGMan.\n",
      "#   This is similar to the regular cumulative job badput wall time, but includes\n",
      "#   job management overhead and delays. In case of job retries the value\n",
      "#   is the cumulative of all retries. For workflows having sub workflow\n",
      "#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs\n",
      "#   from the sub workflows as well.\n",
      "------------------------------------------------------------------------------\n",
      "Type           Succeeded Failed  Incomplete  Total     Retries   Total+Retries\n",
      "Tasks          4         0       0           4         0         4\n",
      "Jobs           25        0       0           25        0         25\n",
      "Sub-Workflows  0         0       0           0         0         0\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Workflow wall time                                       : 6 mins, 40 secs\n",
      "Cumulative job wall time                                 : 5 mins, 51 secs\n",
      "Cumulative job wall time as seen from submit side        : 5 mins, 58 secs\n",
      "Cumulative job badput wall time                          : 0.0 secs\n",
      "Cumulative job badput wall time as seen from submit side : 0.0 secs\n",
      "\n",
      "# Integrity Metrics\n",
      "# Number of files for which checksums were compared/computed along with total time spent doing it.\n",
      "257 files checksums compared with total duration of 9.35 secs\n",
      "114 files checksums generated with total duration of 1.18 secs\n",
      "\n",
      "# Integrity Errors\n",
      "# Total:\n",
      "#       Total number of integrity errors encountered across all job executions(including retries) of a workflow.\n",
      "# Failures:\n",
      "#       Number of failed jobs where the last job instance had integrity errors.\n",
      "Failures: 0 job failures had integrity errors\n",
      "\n",
      "Summary                       : /home/scitech/shared-data/CatsAndDogs/scitech/pegasus/Cats_and_Dogs/run0029/statistics/summary.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "     wf.plan(submit=True)\\\n",
    "        .wait()\\\n",
    "        .analyze()\\\n",
    "        .statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# with open(\"wf-output/checkpoint1.csv\", encoding=\"utf8\") as csvfile:\n",
    "#     csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "\n",
    "#     for row in csvreader:\n",
    "#         print(\": \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with open(\"wf-output/checkpoint1.csv\", 'r', encoding='ISO-8859-1', newline='') as csvfile:\n",
    "#     csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "#     for row in csvreader:\n",
    "#         print(', '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "csvReader = csv.reader(codecs.open(\"wf-output/checkpoint.csv\", 'rU', 'ISO-8859-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "line contains NULL byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-07459f791c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsvReader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: line contains NULL byte"
     ]
    }
   ],
   "source": [
    "for row in csvReader:\n",
    "        print(', '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
