{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "\n",
    "with open('training.pkl', 'rb') as f:\n",
    "     train = pickle.load(f)\n",
    "    \n",
    "with open('testing.pkl', 'rb') as f:\n",
    "     test = pickle.load(f)\n",
    "        \n",
    "with open('validation.pkl','rb') as f:\n",
    "     val = pickle.load(f)\n",
    "        \n",
    "train_photos, train_labels = list(), list()\n",
    "tp = list()\n",
    "for file in train:\n",
    "    if 'Cat' in file:\n",
    "        output = 1.0\n",
    "    else:\n",
    "        output = 0.0\n",
    "    photo = load_img(file)\n",
    "    photo = img_to_array(photo)\n",
    "    train_photos.append(photo)\n",
    "    train_labels.append(output)\n",
    "train_photos = asarray(train_photos)\n",
    "train_labels = asarray(train_labels)\n",
    "\n",
    "test_photos, test_labels = list(), list()\n",
    "for file in test:\n",
    "    if 'Cat' in file:\n",
    "        output = 1.0\n",
    "    else:\n",
    "        output = 0.0\n",
    "    photo = load_img(file)\n",
    "    photo = img_to_array(photo)\n",
    "    tp.append(photo)\n",
    "    test_photos.append(photo)\n",
    "    test_labels.append(output)\n",
    "test_photos = asarray(test_photos)\n",
    "test_labels = asarray(test_labels)\n",
    "\n",
    "val_photos, val_labels = list(), list()\n",
    "for file in val:\n",
    "    if 'Cat' in file:\n",
    "        output = 1.0\n",
    "    else:\n",
    "        output = 0.0\n",
    "    photo = load_img(file)\n",
    "    photo = img_to_array(photo)\n",
    "    val_photos.append(photo)\n",
    "    val_labels.append(output)\n",
    "val_photos = asarray(val_photos)\n",
    "val_labels = asarray(val_labels)\n",
    "\n",
    "model= load_model(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input,Dense,BatchNormalization,Flatten,Dropout,GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import layer_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model,load_model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "N_TRAIN_EXAMPLES = len(train_photos)\n",
    "N_VALID_EXAMPLES = len(val_photos)\n",
    "BATCHSIZE = 1\n",
    "CLASSES = 2\n",
    "EPOCHS = 3\n",
    "STUDY = None\n",
    "N_TRIALS = 10\n",
    "TAR_FILE = \"basicnet_model.tar.gz\"\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "#     clear_session()\n",
    "\n",
    "    (x_train, y_train), (x_valid, y_valid) = (train_photos,train_labels),(val_photos,val_labels)\n",
    "#     img_x, img_y = x_train.shape[1], x_train.shape[2]\n",
    "#     x_train = x_train.reshape(-1, img_x, img_y, 1)[:N_TRAIN_EXAMPLES].astype(\"float32\") / 255\n",
    "#     x_valid = x_valid.reshape(-1, img_x, img_y, 1)[:N_VALID_EXAMPLES].astype(\"float32\") / 255\n",
    "#     y_train = y_train[:N_TRAIN_EXAMPLES]\n",
    "#     y_valid = y_valid[:N_VALID_EXAMPLES]\n",
    "#     input_shape = (img_x, img_y, 1)\n",
    "    vgg16_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "    x = vgg16_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(CLASSES, activation = 'softmax')(x)\n",
    "    model = Model(inputs = vgg16_model.input, outputs = predictions)\n",
    "    \n",
    "    for layer in vgg16_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # We compile our model with a sampled learning rate.\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=RMSprop(lr=lr), metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        shuffle=True,\n",
    "        batch_size=BATCHSIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "    return score[1]\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "def create_classifier(trial):\n",
    "    \n",
    "    # We optimize the numbers of layers and their units.\n",
    "    vgg16_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "    x = vgg16_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(CLASSES, activation = 'softmax')(x)\n",
    "    model = Model(inputs = vgg16_model.input, outputs = predictions)\n",
    "    \n",
    "    for layer in vgg16_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=RMSprop(lr=lr), metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        shuffle=True,\n",
    "        batch_size=BATCHSIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    score = model.evaluate(x_valid, y_valid, verbose=0)\n",
    "    return score[1]\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    classifier = create_classifier(trial)\n",
    "\n",
    "    classifier.train(input_fn=train_input_fn, steps=TRAIN_STEPS)\n",
    "\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "    return float(eval_results[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.study.create_study( direction='maximize')\n",
    "study.optimize(objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10, timeout=600)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "#     print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "#     print(\"  Params: \")\n",
    "#     for key, value in trial.params.items():\n",
    "#         print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Don\\'t woryy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "\n",
    "#     data_transforms  = transforms.Compose([ToTensorRescale()])\n",
    "    train_dataset    = train_photos\n",
    "    val_dataset      = val_photos\n",
    "    train_labels = train_labels\n",
    "    val_labels   = val_labels\n",
    "\n",
    "    return train_dataset,val_dataset,train_labels,val_labels\n",
    "\n",
    "def train(train_dataset, model, criterion, optimizer):\n",
    "\n",
    "    model.train()\n",
    "#     model.to(DEVICE)\n",
    "    running_loss = 0\n",
    "\n",
    "    for sample_batched in train_loader:        \n",
    "        optimizer.zero_grad() \n",
    "        inputs, labels = sample_batched['image'].float(), sample_batched[\"label\"]\n",
    "\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)     \n",
    "        log_ps = model(inputs)\n",
    "        loss   = criterion(log_ps,labels)       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss/len(train_loader)\n",
    "\n",
    "    return model, train_loss\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    accuracy = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample_batched in val_loader:\n",
    "            inputs,labels   = sample_batched[\"image\"].float(), sample_batched[\"label\"]\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            logs_ps = model(inputs)\n",
    "            test_loss += criterion(logs_ps,labels)\n",
    "            ps = torch.exp(logs_ps)\n",
    "            top_ps, top_class = ps.topk(1,dim =1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        test_loss_final = test_loss/len(val_loader)\n",
    "        accuracy_final  = accuracy/len(val_loader)\n",
    "\n",
    "    return model, test_loss_final, accuracy_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(trial):\n",
    "\n",
    "    model = None\n",
    "    params_optim = None\n",
    "\n",
    "    if MODEL == \"vgg16\":\n",
    "        model = PretrainedVGG16().float()\n",
    "        params_optim = model.parameters()\n",
    "    elif MODEL == \"densenet121\":\n",
    "        model = PretrainedDenseNet121().float()\n",
    "        params_optim = model.parameters()\n",
    "    else:\n",
    "        p = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "        model = BasicNet(p)\n",
    "        params_optim = model.parameters()\n",
    "\n",
    "    return model, params_optim\n",
    "\n",
    "def objective(trial):\n",
    "    global CHECKPOINTS_FILES_LIST\n",
    "    #print(\"Performing trail {}\".format(trial.number))\n",
    "\n",
    "    # Generate the model.\n",
    "    model, params_optim = build_model(trial)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(params_optim, lr=lr)\n",
    "\n",
    "    # Get the cat/dog dataset.\n",
    "    train_loader, valid_loader = get_dataloaders()\n",
    "    criterion = nn.NLLLoss()\n",
    "    losses_dict= {'train': {}, 'test': {}, 'accuracy': {}}\n",
    "    start_epoch = 1\n",
    "    accuracy = 0\n",
    "\n",
    "    for epoch in range(start_epoch,EPOCHS+1):\n",
    "        model, train_loss = train(train_loader, model, criterion, optimizer)\n",
    "        model, test_loss, test_accuracy = validate(valid_loader, model, criterion)\n",
    "        current_metrics = [epoch, train_loss, test_loss, test_accuracy]\n",
    "        losses_dict[\"train\"][epoch], losses_dict[\"test\"][epoch] = train_loss, test_loss\n",
    "        losses_dict[\"accuracy\"][epoch] = test_accuracy\n",
    "        accuracy += test_accuracy\n",
    "\n",
    "        trial.report(test_accuracy, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if epoch % 5 == 0 :\n",
    "            CHECKPOINTS_FILES_LIST= save_checkpoint(STUDY,model,optimizer, current_metrics,CHECKPOINTS_FILES_LIST,losses_dict, trial, str(MODEL))\n",
    "#            plot_losses(losses_dict, epoch, trial.number)\n",
    "    accuracy = accuracy/(EPOCHS+1) # TODO\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_study(hpo_checkpoint_file):\n",
    "    global STUDY\n",
    "    load_checkpoint_flag = True\n",
    "    \n",
    "    if len(CHECKPOINTS_FILES_LIST) == 0:\n",
    "        load_checkpoint_flag = False\n",
    "\n",
    "    if load_checkpoint_flag:\n",
    "        #print(\"Loading an existing study...\")\n",
    "        STUDY = joblib.load(\"hpo_study_checkpoint_\" + MODEL + \".pkl\")\n",
    "        #check_status_last_trial()\n",
    "        todo_trials = N_TRIALS - len(STUDY.trials_dataframe())\n",
    "        if todo_trials > 0 :\n",
    "            #print(\"There are {} trials to do out of {}\".format(todo_trials, N_TRIALS))\n",
    "            STUDY.optimize(objective, n_trials=todo_trials, timeout=600, callbacks=[hpo_monitor])\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"This study is finished. Nothing to do.\")\n",
    "    else:\n",
    "        STUDY = optuna.create_study(direction = 'maximize', study_name = MODEL)\n",
    "        STUDY.optimize(objective, n_trials=N_TRIALS, timeout=600, callbacks=[hpo_monitor])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
